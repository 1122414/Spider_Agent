import json
from typing import Dict, Any, Optional, Literal
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

from agent.tools.registry import tool_registry
from agent.prompt_template import build_system_prompt

# =============================================================================
# 1. å®šä¹‰ç»“æ„åŒ–è¾“å‡ºçš„ Schema (è¿™æ˜¯ç¨³å®šæ€§çš„æ ¸å¿ƒ)
# =============================================================================
class AgentDecisionSchema(BaseModel):
    """Agent çš„å†³ç­–ç»“æ„ï¼ŒåŒ…å«æ€è€ƒè¿‡ç¨‹å’Œè¡ŒåŠ¨æŒ‡ä»¤"""
    
    thought: str = Field(
        default="",
        description="ä½ çš„æ€è€ƒè¿‡ç¨‹ï¼šåˆ†æå½“å‰çŠ¶æ€ã€è¯„ä¼°ä¸Šä¸€æ­¥ç»“æœã€å†³å®šä¸‹ä¸€æ­¥è®¡åˆ’ã€‚"
    )
    action: Literal["next", "stop"] = Field(
        ..., 
        description="å†³ç­–è¡ŒåŠ¨ï¼š'next' è¡¨ç¤ºè°ƒç”¨å·¥å…·ç»§ç»­æ‰§è¡Œï¼Œ'stop' è¡¨ç¤ºä»»åŠ¡å·²å®Œæˆæˆ–æ— æ³•ç»§ç»­ã€‚"
    )
    tool_name: Optional[str] = Field(
        None, 
        description="è¦è°ƒç”¨çš„å·¥å…·åç§°ã€‚å¦‚æœ action æ˜¯ 'stop'ï¼Œæ­¤å­—æ®µå¯ç•™ç©ºã€‚"
    )
    parameters: Optional[Dict[str, Any]] = Field(
        default_factory=dict, 
        description="å·¥å…·è°ƒç”¨çš„å‚æ•°å­—å…¸ã€‚å¦‚æœ action æ˜¯ 'stop'ï¼Œæ­¤å­—æ®µå¯ç•™ç©ºã€‚"
    )

class DecisionEngine:
    """
    Agentå†³ç­–å¼•æ“ (Function Calling ç‰ˆ)
    ä½¿ç”¨ LLM çš„ Tool Calling èƒ½åŠ›æ¥ä¿è¯è¾“å‡ºæ ¼å¼çš„ç»å¯¹ç¨³å®šã€‚
    """
    
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        # ç»‘å®šç»“æ„åŒ–è¾“å‡º Schema
        # è¿™ä¸€æ­¥ä¼šè®© LLM å¼ºåˆ¶è¾“å‡ºç¬¦åˆ AgentDecisionSchema çš„ JSON
        self.structured_llm = self.llm.with_structured_output(AgentDecisionSchema)
        self.max_steps = 10
    
    def think_and_act(self, task: str) -> Dict[str, Any]:
        """ReActå†³ç­–å¾ªç¯çš„æ ¸å¿ƒå®ç°"""
        print(f"ğŸ¯ æ”¶åˆ°æ–°ä»»åŠ¡: {task}")
        
        # æ¯æ¬¡æ–°ä»»åŠ¡æ¸…ç©ºå†å²å’Œç¼“å­˜
        tool_registry.history = []
        tool_registry.last_execution_result = None
        
        step_count = 0
        
        while step_count < self.max_steps:
            step_count += 1
            print(f"\nğŸ”„ [Step {step_count}] Agent æ­£åœ¨æ€è€ƒ (Structured)...")
            
            # 1. å‡†å¤‡ä¸Šä¸‹æ–‡
            # æ³¨æ„ï¼šä½¿ç”¨ Structured Output åï¼Œæˆ‘ä»¬ä¾ç„¶éœ€è¦ System Prompt æ¥æè¿°å·¥å…·åŠŸèƒ½ï¼Œ
            # ä½†ä¸å†éœ€è¦è´¹åŠ›åœ°å»æ•™ LLM "å¦‚ä½•è¾“å‡º JSON"ï¼Œå› ä¸ºå®ƒå·²ç»çŸ¥é“äº†ã€‚
            tools_desc = tool_registry.get_tool_description_prompt()
            
            # è·å–æœ€è¿‘æ‰§è¡Œå†å²
            raw_history = tool_registry.get_recent_history(5)
            # å°†å†å²è½¬æ¢ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²æ ¼å¼ï¼Œä¾› LLM å‚è€ƒ
            recent_history_str = json.dumps(raw_history, ensure_ascii=False, indent=2)
            
            # 2. æ„é€  Prompt
            system_prompt_str = build_system_prompt(
                tools_desc=tools_desc,
                recent_history_str=recent_history_str,
                task=task
            )
            
            # 3. è°ƒç”¨ LLM (ä½¿ç”¨ç»“æ„åŒ–æ¨¡å¼)
            try:
                # æ„é€ æ¶ˆæ¯åˆ—è¡¨
                messages = [
                    ("system", system_prompt_str),
                    ("user", f"å½“å‰ä»»åŠ¡çŠ¶æ€å¦‚ä¸Šã€‚è¯·æ ¹æ® {task} è¿›è¡Œä¸‹ä¸€æ­¥å†³ç­–ã€‚")
                ]
                
                # invoke è¿”å›çš„ç›´æ¥æ˜¯ AgentDecisionSchema çš„å®ä¾‹å¯¹è±¡
                decision_obj: AgentDecisionSchema = self.structured_llm.invoke(messages)
                
                # è½¬ä¸ºå­—å…¸æ–¹ä¾¿å¤„ç†  modelå¯¹è±¡çš„å‡½æ•°  ç»§æ‰¿BaseModel
                decision = decision_obj.model_dump()
                
            except Exception as e:
                print(f"âŒ LLM è°ƒç”¨æˆ–è§£æå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return {"status": "error", "message": f"Decision failed: {str(e)}"}
            
            # 4. æ‰§è¡Œå†³ç­–é€»è¾‘
            action = decision.get("action")
            thought = decision.get("thought", "")
            tool_name = decision.get("tool_name")
            params = decision.get("parameters", {})

            print(f"ğŸ’¡ Thought: {thought}")
            
            # --- åˆ†æ”¯ A: åœæ­¢/å®Œæˆ ---
            if action == "stop":
                print("ğŸ ä»»åŠ¡å®Œæˆæˆ–åœæ­¢ã€‚")
                return {
                    "status": "completed",
                    "final_thought": thought,
                    "history": tool_registry.history
                }
            
            # --- åˆ†æ”¯ B: ç»§ç»­æ‰§è¡Œå·¥å…· ---
            elif action == "next":
                if not tool_name:
                    print("âš ï¸ è­¦å‘Š: LLM å†³å®šç»§ç»­ï¼Œä½†æœªæä¾›å·¥å…·åç§°ï¼Œå¼ºåˆ¶é‡è¯•...")
                    continue

                # æ‰§è¡Œå·¥å…·
                result = tool_registry.execute_tool(tool_name, params)
                
                # å°†ç»“æœå†™å…¥å†å²ï¼Œä¾›ä¸‹ä¸€è½®æ€è€ƒä½¿ç”¨
                tool_registry.add_to_history(tool_name, params, result)
                
            else:
                return {"status": "error", "message": f"Invalid action: {action}"}
                
        return {"status": "timeout", "message": "è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶"}

# å…¨å±€å•ä¾‹ç®¡ç†
decision_engine = None

def init_decision_engine(llm: ChatOpenAI):
    global decision_engine
    decision_engine = DecisionEngine(llm)
    return decision_engine