import asyncio
import nest_asyncio
from typing import List, Dict, Any, Set, Union
import re
from urllib.parse import urljoin

# å¼•å…¥åŸç”Ÿ Playwright
from playwright.async_api import async_playwright
from langchain_core.documents import Document 
from langchain_community.document_transformers import Html2TextTransformer
from agent.tools.extractor_agent import ExtractorAgent

# åº”ç”¨ nest_asyncio è¡¥ä¸
nest_asyncio.apply()

# ==========================================
# 1. è¾…åŠ©å·¥å…·å‡½æ•° (Helpers)
# ==========================================

def _normalize_url(url: str) -> str:
    """
    URL æ ‡å‡†åŒ–ï¼Œç”¨äºå»é‡æ¯”è¾ƒã€‚
    """
    if not url:
        return ""
    return url.strip().rstrip("/")

async def _auto_scroll(page, max_scrolls: int):
    """
    æ¨¡æ‹Ÿäººå·¥æ»šåŠ¨ä»¥è§¦å‘æ‡’åŠ è½½
    """
    if max_scrolls <= 0:
        return
    print(f"   Start auto-scroll (Max: {max_scrolls})...")
    for i in range(max_scrolls):
        try:
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(1500) 
            # print(f"   Scrolled {i+1}/{max_scrolls}")
        except Exception as e:
            print(f"   Scroll failed: {e}")
            break

# ==========================================
# 2. æ ¸å¿ƒå¼‚æ­¥é€»è¾‘ (Async Core Functions)
# ==========================================

async def playwright_fetch(
    url: str, 
    target: List[str], 
    wait: float = 2.0, 
    max_scrolls: int = 0
) -> Dict:
    """
    [åŸºç¡€çˆ¬è™«] ä½¿ç”¨ Playwright æå–å•é¡µé¢å†…å®¹
    """
    print(f"ğŸ•·ï¸ Fetching: {url}")
    
    raw_html = ""
    error_msg = None

    try:
        async with async_playwright() as p:
            # ç”Ÿäº§ç¯å¢ƒå»ºè®® headless=True
            browser = await p.chromium.launch(headless=False) 
            
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
            )
            
            page = await context.new_page()

            try:
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                
                if max_scrolls > 0:
                    await _auto_scroll(page, max_scrolls)
                else:
                    await page.wait_for_timeout(wait * 1000)

            except Exception as e:
                print(f"âš ï¸ Page load warning: {e}")
            
            raw_html = await page.content()
            await browser.close()

    except Exception as e:
        error_msg = f"Playwright Critical Error: {str(e)}"
        return {"url": url, "error": error_msg}

    if not raw_html:
        return {"url": url, "error": "Failed to load content"}

    # --- æ•°æ®æ¸…æ´—ä¸æå– ---
    docs = [Document(page_content=raw_html, metadata={"source": url})]
    html2text = Html2TextTransformer(ignore_links=False)
    transformed_docs = html2text.transform_documents(docs)
    pure_text = transformed_docs[0].page_content if transformed_docs else ""

    match = re.search(r"<title>(.*?)</title>", raw_html, re.S | re.I)
    title = match.group(1).strip() if match else "No Title"

    try:
        extractor = ExtractorAgent()
        target_content = extractor.get_content(pure_text, target, url)
    except Exception as e:
        target_content = f"Extraction Failed: {str(e)}"

    return {
        "url": url,
        "title": title,
        "target_content": target_content
    }

async def _recursive_crawl_logic(
    current_url: str,
    pipelines: List[List[str]], # æ¯ä¸€å±‚çš„æå–ç›®æ ‡
    current_depth: int,
    max_items: int,
    visited_urls: Set[str]
) -> Union[List[Dict], Dict, str]:
    """
    [å†…éƒ¨é€’å½’å‡½æ•°] å¤„ç†å¤šå±‚çº§çˆ¬å–é€»è¾‘
    """
    # 1. è¾¹ç•Œæ£€æŸ¥
    if current_depth >= len(pipelines):
        return None # è¶…è¿‡é¢„è®¾æ·±åº¦ï¼Œåœæ­¢

    target = pipelines[current_depth]
    # åªæœ‰åˆ—è¡¨é¡µæ‰éœ€è¦æ»šåŠ¨ï¼Œè¯¦æƒ…é¡µé€šå¸¸ä¸éœ€è¦
    scrolls = 1 if current_depth == 0 else 0 
    
    # 2. çˆ¬å–å½“å‰å±‚
    # è‡ªåŠ¨ç»™æ¯ä¸€å±‚åŠ ä¸Š "link" ç›¸å…³çš„æå–æç¤ºï¼Œæ–¹ä¾¿ä¸‹ä¸€å±‚é’»å–
    enhanced_target = target + ["link", "url", "href", "é“¾æ¥", "è·³è½¬é“¾æ¥"]
    
    fetch_result = await playwright_fetch(current_url, enhanced_target, max_scrolls=scrolls)
    
    if "error" in fetch_result and fetch_result["error"]:
        return {"error": fetch_result["error"], "url": current_url}

    extracted_data = fetch_result.get("target_content")

    # 3. å¦‚æœæ˜¯æœ€åä¸€å±‚ï¼Œç›´æ¥è¿”å›æ•°æ®
    if current_depth == len(pipelines) - 1:
        return extracted_data

    # 4. å‡†å¤‡è¿›å…¥ä¸‹ä¸€å±‚
    # å…¼å®¹å¤„ç†ï¼šå¦‚æœæå–ç»“æœæ˜¯å•ä¸ªå­—å…¸ï¼Œè½¬ä¸ºåˆ—è¡¨ç»Ÿä¸€å¤„ç†
    items = []
    if isinstance(extracted_data, list):
        items = extracted_data
    elif isinstance(extracted_data, dict):
        items = [extracted_data]
    else:
        # å¦‚æœæå–ç»“æœæ˜¯çº¯æ–‡æœ¬æˆ–å…¶ä»–ï¼Œæ— æ³•ç»§ç»­æ·±å…¥ï¼Œç›´æ¥è¿”å›
        return extracted_data

    # 5. éå†å½“å‰å±‚æ¡ç›®ï¼Œå¯»æ‰¾é“¾æ¥è¿›å…¥ä¸‹ä¸€å±‚
    results = []
    count = 0
    
    # å¸¸è§çš„é“¾æ¥å­—æ®µå
    link_keys = ["link", "url", "href", "é“¾æ¥", "è¯¦æƒ…é¡µé“¾æ¥", "çº¿è·¯é“¾æ¥", "æ’­æ”¾é“¾æ¥", "full_url"]

    for item in items:
        # è¶…è¿‡æœ€å¤§æ•°é‡é™åˆ¶åˆ™åœæ­¢æœ¬å±‚éå†
        if count >= max_items:
            break
            
        processed_item = item.copy() if isinstance(item, dict) else {"raw": item}
        
        # A. å¯»æ‰¾ä¸‹ä¸€å±‚é“¾æ¥
        next_url = None
        if isinstance(item, dict):
            for key in link_keys:
                if key in item and item[key] and isinstance(item[key], str):
                    candidate = item[key].strip()
                    if len(candidate) > 1:
                        next_url = candidate
                        break
        
        # B. å¦‚æœæ‰¾åˆ°é“¾æ¥ï¼Œé€’å½’é’»å–
        if next_url:
            # æ‹¼æ¥å®Œæ•´ URL
            full_next_url = urljoin(current_url, next_url)
            normalized_next = _normalize_url(full_next_url)

            if normalized_next not in visited_urls:
                print(f"   ğŸ‘‰ [Depth {current_depth}->{current_depth+1}] Digging: {full_next_url}")
                visited_urls.add(normalized_next)
                
                # ã€é€’å½’è°ƒç”¨ã€‘
                sub_data = await _recursive_crawl_logic(
                    full_next_url, 
                    pipelines, 
                    current_depth + 1, 
                    max_items, 
                    visited_urls
                )
                
                # å°†ä¸‹ä¸€å±‚æ•°æ®æŒ‚è½½åˆ°å½“å‰ item çš„ "children" å­—æ®µ
                # æˆ–è€…å¦‚æœä¸‹ä¸€å±‚è¿”å›çš„æ˜¯å­—å…¸ï¼ˆåˆå¹¶ï¼‰ï¼Œè§†æƒ…å†µè€Œå®šã€‚è¿™é‡Œç»Ÿä¸€æŒ‚åœ¨ children ä¸‹ç»“æ„æœ€æ¸…æ™°ã€‚
                processed_item["children"] = sub_data
                count += 1
            else:
                processed_item["info"] = "URL visited or repeated"
        
        results.append(processed_item)

    return results

async def hierarchical_crawl(
    url: str, 
    crawl_scopes: List[List[str]], 
    max_items: int = 3
) -> Dict:
    """
    [å¤šå±‚çº§æ·±åº¦çˆ¬è™« - å¼‚æ­¥å…¥å£]
    å‚æ•°:
      url: èµ·å§‹ URL
      crawl_scopes: æ¯ä¸€å±‚çš„æå–ç›®æ ‡åˆ—è¡¨ã€‚
         ä¾‹å¦‚: [ ["åŠ¨æ¼«æ ‡é¢˜", "é“¾æ¥"], ["çº¿è·¯é“¾æ¥", "çº¿è·¯å"], ["è¯„è®º", "è§†é¢‘æ ‡é¢˜"] ]
      max_items: æ¯ä¸€å±‚æœ€å¤§æŠ“å–æ•°é‡ï¼ˆé˜²æ­¢æŒ‡æ•°çº§çˆ†ç‚¸ï¼‰
    """
    print(f"ğŸš€ [Multi-Level] å¯åŠ¨å¤šå±‚çˆ¬å–: {url}")
    print(f"   Pipeline Depth: {len(crawl_scopes)} å±‚")

    visited_urls = set()
    visited_urls.add(_normalize_url(url))

    # å¼€å§‹é€’å½’
    final_data = await _recursive_crawl_logic(
        url, 
        crawl_scopes, 
        current_depth=0, 
        max_items=max_items, 
        visited_urls=visited_urls
    )

    return {
        "root_url": url,
        "depth_configured": len(crawl_scopes),
        "data": final_data
    }

# ==========================================
# 3. åŒæ­¥åŒ…è£…å™¨ (Sync Wrappers)
# ==========================================

def _run_async(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        return loop.run_until_complete(coro)
    else:
        return asyncio.run(coro)

def sync_playwright_fetch(url: str, target: List[str], max_scrolls: int = 0) -> Dict:
    """åŸºç¡€çˆ¬è™«å…¥å£"""
    return _run_async(playwright_fetch(url, target, max_scrolls=max_scrolls))

def sync_hierarchical_crawl(url: str, crawl_scopes: List[List[str]], max_items: int = 3) -> Dict:
    """
    å¤šå±‚çº§çˆ¬è™«å…¥å£ï¼Œæ”¯æŒä»»æ„å±‚çº§
    """
    return _run_async(hierarchical_crawl(url, crawl_scopes, max_items))