import asyncio
import nest_asyncio
from typing import List, Dict, Any, Set, Union
import re
from urllib.parse import urljoin

# å¼•å…¥åŸç”Ÿ Playwright
from playwright.async_api import async_playwright
from langchain_core.documents import Document 
from langchain_community.document_transformers import Html2TextTransformer
from agent.tools.extractor_agent import ExtractorAgent

# åº”ç”¨ nest_asyncio è¡¥ä¸
nest_asyncio.apply()

# ==========================================
# 1. è¾…åŠ©å·¥å…·å‡½æ•° (Helpers)
# ==========================================

def _normalize_url(url: str) -> str:
    """
    URL æ ‡å‡†åŒ–ï¼Œç”¨äºå»é‡æ¯”è¾ƒã€‚
    """
    if not url:
        return ""
    return url.strip().rstrip("/")

async def _auto_scroll(page, max_scrolls: int):
    """
    æ¨¡æ‹Ÿäººå·¥æ»šåŠ¨ä»¥è§¦å‘æ‡’åŠ è½½
    """
    if max_scrolls <= 0:
        return
    print(f"   Start auto-scroll (Max: {max_scrolls})...")
    for i in range(max_scrolls):
        try:
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(1500) 
            # print(f"   Scrolled {i+1}/{max_scrolls}")
        except Exception as e:
            print(f"   Scroll failed: {e}")
            break

# ==========================================
# 2. æ ¸å¿ƒå¼‚æ­¥é€»è¾‘ (Async Core Functions)
# ==========================================

async def playwright_fetch(
    url: str, 
    target: List[str], 
    wait: float = 2.0, 
    max_scrolls: int = 0
) -> Dict:
    """
    [åŸºç¡€çˆ¬è™«] ä½¿ç”¨ Playwright æå–å•é¡µé¢å†…å®¹
    è¿”å›ç»“æ„åŒ…å« extractor çš„åŸå§‹è¿”å› (items + next_page_url)
    """
    print(f"ğŸ•·ï¸ Fetching: {url}")
    
    raw_html = ""
    error_msg = None

    try:
        async with async_playwright() as p:
            # ç”Ÿäº§ç¯å¢ƒå»ºè®® headless=True
            browser = await p.chromium.launch(headless=False) 
            
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
            )
            
            page = await context.new_page()

            try:
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                
                if max_scrolls > 0:
                    await _auto_scroll(page, max_scrolls)
                else:
                    await page.wait_for_timeout(wait * 1000)

            except Exception as e:
                print(f"âš ï¸ Page load warning: {e}")
            
            raw_html = await page.content()
            await browser.close()

    except Exception as e:
        error_msg = f"Playwright Critical Error: {str(e)}"
        return {"url": url, "error": error_msg}

    if not raw_html:
        return {"url": url, "error": "Failed to load content"}

    # --- æ•°æ®æ¸…æ´—ä¸æå– ---
    docs = [Document(page_content=raw_html, metadata={"source": url})]
    html2text = Html2TextTransformer(ignore_links=False)
    transformed_docs = html2text.transform_documents(docs)
    pure_text = transformed_docs[0].page_content if transformed_docs else ""

    match = re.search(r"<title>(.*?)</title>", raw_html, re.S | re.I)
    title = match.group(1).strip() if match else "No Title"

    try:
        extractor = ExtractorAgent()
        # ExtractorAgent ç°åœ¨è¿”å› {"items": [...], "next_page_url": ...}
        target_content = extractor.get_content(pure_text, target, url)
    except Exception as e:
        target_content = {"items": [], "next_page_url": None, "error": str(e)}

    return {
        "url": url,
        "title": title,
        "target_content": target_content
    }

async def _recursive_crawl_logic(
    start_url: str,
    pipelines: List[List[str]], # æ¯ä¸€å±‚çš„æå–ç›®æ ‡
    current_depth: int,
    max_items: int,
    visited_urls: Set[str],
    max_pages: int = 3  # æ–°å¢ï¼šæœ€å¤§ç¿»é¡µæ•°
) -> Union[List[Dict], Dict, str]:
    """
    [å†…éƒ¨é€’å½’å‡½æ•°] å¤„ç†å¤šå±‚çº§çˆ¬å–é€»è¾‘ï¼Œæ”¯æŒç¿»é¡µ
    """
    # 1. è¾¹ç•Œæ£€æŸ¥
    if current_depth >= len(pipelines):
        return None 

    target = pipelines[current_depth]
    # åªæœ‰åˆ—è¡¨é¡µ(Depth 0)æˆ–æ˜ç¡®éœ€è¦ç¿»é¡µçš„å±‚çº§æ‰æ»šåŠ¨
    scrolls = 1 
    
    # è‡ªåŠ¨ç»™æ¯ä¸€å±‚åŠ ä¸Šé“¾æ¥æå–æç¤º
    enhanced_target = target + ["link", "url", "href", "é“¾æ¥", "è·³è½¬é“¾æ¥"]
    
    all_layer_results = []
    current_page_url = start_url
    page_count = 0

    # ============================
    # åˆ†é¡µå¾ªç¯ (Pagination Loop)
    # ============================
    while current_page_url and page_count < max_pages:
        # å»é‡æ£€æŸ¥ (é’ˆå¯¹åˆ—è¡¨é¡µæœ¬èº«)
        normalized_current = _normalize_url(current_page_url)
        if normalized_current in visited_urls:
             print(f"   âš ï¸ [Depth {current_depth}] Page visited, stopping pagination: {current_page_url}")
             break
        visited_urls.add(normalized_current)

        if page_count > 0:
            print(f"   ğŸ“„ [Depth {current_depth}] Flipping to Page {page_count + 1}: {current_page_url}")

        # 2. çˆ¬å–å½“å‰é¡µ
        fetch_result = await playwright_fetch(current_page_url, enhanced_target, max_scrolls=scrolls)
        
        if "error" in fetch_result and fetch_result["error"]:
            print(f"   âŒ Fetch error at {current_page_url}: {fetch_result['error']}")
            break

        extracted_data = fetch_result.get("target_content", {})
        
        # å…¼å®¹æ€§å¤„ç†ï¼šç¡®ä¿æ‹¿åˆ° items åˆ—è¡¨å’Œ next_page_url
        items = []
        next_link = None

        if isinstance(extracted_data, dict):
            items = extracted_data.get("items", [])
            next_link = extracted_data.get("next_page_url")
            # å¦‚æœæ—§ç‰ˆ extractor è¿”å›äº† content æ”¾åœ¨å…¶ä»–å­—æ®µï¼Œåšä¸ªå…¼å®¹ï¼ˆè§† Extractor å®ç°è€Œå®šï¼‰
        elif isinstance(extracted_data, list):
            items = extracted_data # æ—§ç‰ˆå…¼å®¹
        
        # 3. å¤„ç†å½“å‰é¡µçš„ items
        # å¦‚æœä¸æ˜¯æœ€åä¸€å±‚ï¼Œéœ€è¦é€’å½’æ·±å…¥
        if current_depth < len(pipelines) - 1:
            processed_items = await _process_items_recursively(
                items, 
                current_page_url, 
                pipelines, 
                current_depth, 
                max_items, 
                visited_urls,
                max_pages
            )
            all_layer_results.extend(processed_items)
        else:
            # æœ€åä¸€å±‚ï¼Œç›´æ¥æ”¶é›†æ•°æ®
            all_layer_results.extend(items)

        # 4. å‡†å¤‡ä¸‹ä¸€é¡µ
        if next_link:
            # æ‹¼æ¥å®Œæ•´ URL
            next_full_url = urljoin(current_page_url, next_link)
            
            # é˜²æ­¢åŸåœ°è¸æ­¥
            if _normalize_url(next_full_url) == normalized_current:
                print("   âš ï¸ Next page is same as current, stopping.")
                break
                
            current_page_url = next_full_url
            page_count += 1
        else:
            # æ²¡æœ‰ä¸‹ä¸€é¡µäº†
            break
    
    return all_layer_results

async def _process_items_recursively(
    items: List[Dict], 
    base_url: str,
    pipelines: List[List[str]],
    current_depth: int,
    max_items: int,
    visited_urls: Set[str],
    max_pages: int
) -> List[Dict]:
    """
    è¾…åŠ©å‡½æ•°ï¼šéå† items å¹¶é€’å½’è°ƒç”¨ä¸‹ä¸€å±‚
    """
    results = []
    count = 0
    link_keys = ["link", "url", "href", "é“¾æ¥", "è¯¦æƒ…é¡µé“¾æ¥", "çº¿è·¯é“¾æ¥", "æ’­æ”¾é“¾æ¥", "full_url"]

    for item in items:
        if not isinstance(item, dict):
            results.append({"raw": item})
            continue
            
        if count >= max_items:
            break
            
        processed_item = item.copy()
        
        # A. å¯»æ‰¾ä¸‹ä¸€å±‚é“¾æ¥
        next_url = None
        for key in link_keys:
            if key in item and item[key] and isinstance(item[key], str):
                candidate = item[key].strip()
                if len(candidate) > 1:
                    next_url = candidate
                    break
        
        # B. é€’å½’é’»å–
        if next_url:
            full_next_url = urljoin(base_url, next_url)
            normalized_next = _normalize_url(full_next_url)

            if normalized_next not in visited_urls:
                print(f"   ğŸ‘‰ [Depth {current_depth}->{current_depth+1}] Digging: {full_next_url}")
                # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦æŠŠè¯¦æƒ…é¡µåŠ å…¥ visited_urls ä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œå–å†³äºæ˜¯å¦å…è®¸ä¸åŒåˆ—è¡¨é¡¹æŒ‡å‘åŒä¸€è¯¦æƒ…é¡µ
                # è¿™é‡ŒåŠ å…¥æ˜¯ä¸ºäº†é˜²ç¯
                visited_urls.add(normalized_next)
                
                sub_data = await _recursive_crawl_logic(
                    full_next_url, 
                    pipelines, 
                    current_depth + 1, 
                    max_items, 
                    visited_urls,
                    max_pages
                )
                
                processed_item["children"] = sub_data
                count += 1
            else:
                processed_item["info"] = "URL visited or repeated"
        
        results.append(processed_item)
    
    return results

async def hierarchical_crawl(
    url: str, 
    crawl_scopes: List[List[str]], 
    max_items: int = 3,
    max_pages: int = 3
) -> Dict:
    """
    [å¤šå±‚çº§æ·±åº¦çˆ¬è™« - å¼‚æ­¥å…¥å£]
    å‚æ•°:
      url: èµ·å§‹ URL
      crawl_scopes: æå–ç›®æ ‡äºŒç»´æ•°ç»„
      max_items: æ¯ä¸€å±‚é€’å½’æŠ“å–çš„æœ€å¤§æ¡ç›®æ•°
      max_pages: æ¯ä¸€å±‚åˆ—è¡¨é¡µçš„æœ€å¤§ç¿»é¡µæ•°
    """
    print(f"ğŸš€ [Multi-Level] å¯åŠ¨å¤šå±‚çˆ¬å–: {url}")
    print(f"   Pipeline Depth: {len(crawl_scopes)} å±‚ | Max Pages: {max_pages}")

    visited_urls = set()
    # visited_urls.add(_normalize_url(url)) # ç§»åˆ°é€’å½’å†…éƒ¨å¤„ç†ï¼Œé˜²æ­¢ç¬¬ä¸€é¡µå°±è¢«è·³è¿‡

    # å¼€å§‹é€’å½’
    final_data = await _recursive_crawl_logic(
        url, 
        pipelines=crawl_scopes, 
        current_depth=0, 
        max_items=max_items, 
        visited_urls=visited_urls,
        max_pages=max_pages
    )

    return {
        "root_url": url,
        "depth_configured": len(crawl_scopes),
        "data": final_data
    }

# ==========================================
# 3. åŒæ­¥åŒ…è£…å™¨ (Sync Wrappers)
# ==========================================

def _run_async(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        return loop.run_until_complete(coro)
    else:
        return asyncio.run(coro)

def sync_playwright_fetch(url: str, target: List[str], max_scrolls: int = 0) -> Dict:
    """åŸºç¡€çˆ¬è™«å…¥å£"""
    return _run_async(playwright_fetch(url, target, max_scrolls=max_scrolls))

def sync_hierarchical_crawl(url: str, crawl_scopes: List[List[str]], max_items: int = 3, max_pages: int = 3) -> Dict:
    """
    [æ–°ç‰ˆ] å¤šå±‚çº§çˆ¬è™«å…¥å£ï¼Œæ”¯æŒç¿»é¡µå‚æ•°
    """
    return _run_async(hierarchical_crawl(url, crawl_scopes, max_items, max_pages))