import re
import random
import asyncio
import nest_asyncio
import os
from typing import List, Dict, Any, Set, Union, Optional
from urllib.parse import urljoin

# å¼•å…¥åŸç”Ÿ Playwright
from playwright.async_api import async_playwright, BrowserContext, Page
from langchain_core.documents import Document 
from langchain_community.document_transformers import Html2TextTransformer

# è‡ªå®šä¹‰ Agent
from agent.tools.extractor_agent import ExtractorAgent

# åº”ç”¨ nest_asyncio è¡¥ä¸ (é˜²æ­¢ Jupyter/EventLoop å†²çª)
nest_asyncio.apply()

# ==========================================
# 1. æŒä¹…åŒ–çˆ¬è™«ç±» (Persistent Fetcher)
# ==========================================

class PersistentFetcher:
    def __init__(self, user_data_dir: str = "./browser_data", headless: bool = False):
        """
        åˆå§‹åŒ–æŒä¹…åŒ–çˆ¬è™«
        :param user_data_dir: æµè§ˆå™¨æ•°æ®å­˜å‚¨è·¯å¾„ (Cookies/Cacheå°†ä¿å­˜åœ¨æ­¤)
        :param headless: æ˜¯å¦æ— å¤´æ¨¡å¼
        """
        self.user_data_dir = user_data_dir
        self.headless = headless
        self.playwright = None
        self.context: Optional[BrowserContext] = None
        
        # åˆå§‹åŒ–å·¥å…· (é¿å…æ¯æ¬¡ fetch éƒ½é‡æ–°åˆ›å»º)
        self.html2text = Html2TextTransformer(ignore_links=False)
        # å¦‚æœ ExtractorAgent æœ‰çŠ¶æ€æˆ–åˆå§‹åŒ–å¼€é”€å¤§ï¼Œå»ºè®®æ”¾åœ¨è¿™é‡Œ
        self.extractor = ExtractorAgent() 

    async def start(self):
        """å¯åŠ¨æµè§ˆå™¨å¹¶åŠ è½½æŒä¹…åŒ–ä¸Šä¸‹æ–‡"""
        if not self.playwright:
            print(f"ğŸš€ Starting persistent browser in: {self.user_data_dir}")
            self.playwright = await async_playwright().start()
            
            # ä½¿ç”¨ launch_persistent_context è‡ªåŠ¨ä¿å­˜çŠ¶æ€
            self.context = await self.playwright.chromium.launch_persistent_context(
                user_data_dir=self.user_data_dir,
                headless=self.headless,
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
                viewport={"width": 1280, "height": 800},
                args=["--disable-blink-features=AutomationControlled"]
            )

    async def stop(self):
        """å…³é—­æµè§ˆå™¨èµ„æº"""
        if self.context:
            print("ğŸ›‘ Closing browser context...")
            await self.context.close()
            self.context = None
        if self.playwright:
            await self.playwright.stop()
            self.playwright = None

    async def _auto_scroll(self, page: Page, max_scrolls: int):
        """æ¨¡æ‹Ÿäººå·¥æ»šåŠ¨ä»¥è§¦å‘æ‡’åŠ è½½"""
        if max_scrolls <= 0:
            return
        print(f"   Start auto-scroll (Max: {max_scrolls})...")
        for i in range(max_scrolls):
            try:
                await page.evaluate("window.scrollBy(0, document.body.scrollHeight)")
                # éšæœºç­‰å¾…ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
                await page.wait_for_timeout(random.randint(2000, 5000))
            except Exception as e:
                print(f"   Scroll failed: {e}")
                break

    async def fetch(self, url: str, target: List[str], wait: float = 2.0, max_scrolls: int = 0, wait_time: int = 1000, max_nodes: int = 200) -> Dict:
        """
        æ‰§è¡Œå•é¡µé¢æŠ“å– (å¤ç”¨å·²æ‰“å¼€çš„æµè§ˆå™¨)
        """
        if not self.context:
            await self.start()

        print(f"ğŸ•·ï¸ Fetching: {url}")
        
        # åˆ›å»ºæ–°æ ‡ç­¾é¡µè€Œä¸æ˜¯æ–°æµè§ˆå™¨
        page = await self.context.new_page() 
        
        raw_html = ""
        error_msg = None
        target_content = {}

        try:
            try:
                # è®¾ç½®é¡µé¢åŠ è½½è¶…æ—¶
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)

                await page.wait_for_timeout(random.randint(wait_time, wait_time + 3000))
                
                if max_scrolls > 0:
                    await self._auto_scroll(page, max_scrolls)
                else:
                    await page.wait_for_timeout(wait * 1000)

                raw_html = await page.content()

            except Exception as e:
                print(f"âš ï¸ Page load warning: {e}")
                # å³ä½¿æŠ¥é”™ï¼Œå°è¯•è·å–å·²åŠ è½½çš„å†…å®¹
                raw_html = await page.content()

        except Exception as e:
            error_msg = f"Playwright Critical Error: {str(e)}"
        
        finally:
            # å…³é”®ï¼šåªå…³é—­ Pageï¼Œä¸å…³é—­ Context
            await page.close()
            pass

        if error_msg:
            return {"url": url, "error": error_msg}

        if not raw_html:
            return {"url": url, "error": "Failed to load content"}

        # --- æ•°æ®æ¸…æ´—ä¸æå– ---
        docs = [Document(page_content=raw_html, metadata={"source": url})]

        # extrator_agent_bck åŸç‰ˆä½¿ç”¨
        # transformed_docs = self.html2text.transform_documents(docs)
        # pure_text = transformed_docs[0].page_content if transformed_docs else ""

        # extrator_agent æ–°ç‰ˆ
        match = re.search(r"<title>(.*?)</title>", raw_html, re.S | re.I)
        title = match.group(1).strip() if match else "No Title"

        try:
            # ä½¿ç”¨ç±»æˆå‘˜ extractor
            # ExtractorAgent è¿”å› {"items": [...], "next_page_url": ...}
            # æ—§ç‰ˆ
            # target_content = self.extractor.get_content(pure_text, target, url)
            # æ–°ç‰ˆï¼ˆæµ‹è¯•ä¸­ï¼Œ12.8ï¼‰
            target_content = self.extractor.get_content(raw_html, target, url, max_nodes=max_nodes)
        except Exception as e:
            target_content = {"items": [], "next_page_url": None, "error": str(e)}

        return {
            "url": url,
            "title": title,
            "target_content": target_content
        }

# ==========================================
# 2. è¾…åŠ©å‡½æ•° (Helpers)
# ==========================================

def _normalize_url(url: str) -> str:
    if not url:
        return ""
    return url.strip().rstrip("/")

# ==========================================
# 3. æ ¸å¿ƒé€’å½’é€»è¾‘ (Refactored for PersistentFetcher)
# ==========================================

async def _recursive_crawl_logic(
    start_url: str,
    pipelines: List[List[str]],
    current_depth: int,
    max_items: int,
    visited_urls: Set[str],
    fetcher: PersistentFetcher,  # æ¥æ”¶ fetcher å®ä¾‹
    max_pages: int = 3,
    max_scrolls: int = 1,
    wait_time : int = 1000,
    max_nodes: int = 200
) -> Union[List[Dict], Dict, str]:
    """
    [å†…éƒ¨é€’å½’å‡½æ•°] å¤„ç†å¤šå±‚çº§çˆ¬å–é€»è¾‘ï¼Œæ”¯æŒç¿»é¡µ
    """
    # 1. è¾¹ç•Œæ£€æŸ¥
    if current_depth >= len(pipelines):
        return None 

    target = pipelines[current_depth]
    
    # è‡ªåŠ¨ç»™æ¯ä¸€å±‚åŠ ä¸Šé“¾æ¥æå–æç¤º
    enhanced_target = target + ["link", "url", "href", "é“¾æ¥", "è·³è½¬é“¾æ¥"]
    
    all_layer_results = []
    current_page_url = start_url
    page_count = 0

    # ============================
    # åˆ†é¡µå¾ªç¯ (Pagination Loop)
    # ============================
    while current_page_url and page_count < max_pages:
        # å»é‡æ£€æŸ¥
        normalized_current = _normalize_url(current_page_url)
        if normalized_current in visited_urls:
             print(f"   âš ï¸ [Depth {current_depth}] Page visited, stopping pagination: {current_page_url}")
             break
        visited_urls.add(normalized_current)

        if page_count > 0:
            print(f"   ğŸ“„ [Depth {current_depth}] Flipping to Page {page_count + 1}: {current_page_url}")

        # 2. çˆ¬å–å½“å‰é¡µ (è°ƒç”¨ fetcher å®ä¾‹æ–¹æ³•)
        fetch_result = await fetcher.fetch(current_page_url, enhanced_target, max_scrolls=max_scrolls, wait_time=wait_time, max_nodes=max_nodes)

        if "error" in fetch_result and fetch_result["error"]:
            print(f"   âŒ Fetch error at {current_page_url}: {fetch_result['error']}")
            break

        extracted_data = fetch_result.get("target_content", {})
        
        items = []
        next_link = None

        if isinstance(extracted_data, dict):
            items = extracted_data.get("items", [])
            next_link = extracted_data.get("next_page_url")
        elif isinstance(extracted_data, list):
            items = extracted_data # æ—§ç‰ˆå…¼å®¹
        
        # 3. å¤„ç†å½“å‰é¡µçš„ items (é€’å½’å…¥å£)
        if current_depth < len(pipelines) - 1:
            processed_items = await _process_items_recursively(
                items, 
                current_page_url, 
                pipelines, 
                current_depth, 
                max_items, 
                visited_urls,
                fetcher, # ä¼ é€’ fetcher
                max_pages
            )
            all_layer_results.extend(processed_items)
        else:
            # æœ€åä¸€å±‚ï¼Œç›´æ¥æ”¶é›†æ•°æ®
            all_layer_results.extend(items)

        # 4. å‡†å¤‡ä¸‹ä¸€é¡µ
        if not next_link or not isinstance(next_link, str):
            print(f"[Warning] è·³è¿‡æ— æ•ˆé“¾æ¥: {next_link}")
            continue  # æˆ–è€… returnï¼Œå–å†³äºä½ çš„å¾ªç¯ç»“æ„

        # 2. æ£€æŸ¥ current_page_url (è™½ç„¶å¯èƒ½æ€§è¾ƒå°ï¼Œä½†ä¹Ÿå¯èƒ½æ˜¯ None)
        if not current_page_url or not isinstance(current_page_url, str):
            print(f"[Error] å½“å‰é¡µé¢ URL æ— æ•ˆ: {current_page_url}")
            continue
        
        if next_link:
            next_full_url = urljoin(current_page_url, next_link)
            if _normalize_url(next_full_url) == normalized_current:
                print("   âš ï¸ Next page is same as current, stopping.")
                break
            current_page_url = next_full_url
            page_count += 1
        else:
            break
    
    return all_layer_results

async def _process_items_recursively(
    items: List[Dict], 
    base_url: str,
    pipelines: List[List[str]],
    current_depth: int,
    max_items: int,
    visited_urls: Set[str],
    fetcher: PersistentFetcher, # æ¥æ”¶ fetcher
    max_pages: int
) -> List[Dict]:
    """
    è¾…åŠ©å‡½æ•°ï¼šéå† items å¹¶é€’å½’è°ƒç”¨ä¸‹ä¸€å±‚
    """
    results = []
    count = 0
    link_keys = ["link", "url", "href", "é“¾æ¥", "è¯¦æƒ…é¡µé“¾æ¥", "çº¿è·¯é“¾æ¥", "æ’­æ”¾é“¾æ¥", "full_url"]

    for item in items:
        if not isinstance(item, dict):
            results.append({"raw": item})
            continue
            
        if count >= max_items:
            break
            
        processed_item = item.copy()
        
        # A. å¯»æ‰¾ä¸‹ä¸€å±‚é“¾æ¥
        next_url = None
        for key in link_keys:
            if key in item and item[key] and isinstance(item[key], str):
                candidate = item[key].strip()
                if len(candidate) > 1:
                    next_url = candidate
                    break
        
        # B. é€’å½’é’»å–
        if next_url:
            full_next_url = urljoin(base_url, next_url)
            normalized_next = _normalize_url(full_next_url)

            if normalized_next not in visited_urls:
                print(f"   ğŸ‘‰ [Depth {current_depth}->{current_depth+1}] Digging: {full_next_url}")
                
                sub_data = await _recursive_crawl_logic(
                    full_next_url, 
                    pipelines, 
                    current_depth + 1, 
                    max_items, 
                    visited_urls,
                    fetcher, # ä¼ é€’ fetcher
                    max_pages
                )

                # é˜²æ­¢æ­»å¾ªç¯ï¼Œå°†è¯¦æƒ…é¡µä¹ŸåŠ å…¥ visited
                visited_urls.add(normalized_next)
                
                processed_item["children"] = sub_data
                count += 1
            else:
                processed_item["info"] = "URL visited or repeated"
        
        results.append(processed_item)
    
    return results

# ==========================================
# 4. å¼‚æ­¥å…¥å£ (Entry Point)
# ==========================================

async def hierarchical_crawl(
    url: str, 
    crawl_scopes: List[List[str]], 
    max_items: int = 3,
    max_pages: int = 3,
    max_scrolls: int = 1,
    headless: bool = False, # æš´éœ² headless å‚æ•°
    wait_time: int = 1000,
    max_nodes: int = 200
) -> Dict:
    """
    [å¤šå±‚çº§æ·±åº¦çˆ¬è™« - å¼‚æ­¥å…¥å£]
    """
    print(f"ğŸš€ [Multi-Level] å¯åŠ¨å¤šå±‚çˆ¬å–: {url}")
    print(f"   Pipeline Depth: {len(crawl_scopes)} å±‚ | Max Pages: {max_pages}")

    # 1. åˆå§‹åŒ– Fetcher
    fetcher = PersistentFetcher(headless=headless)
    
    visited_urls = set()
    final_data = []

    try:
        # 2. å¯åŠ¨æµè§ˆå™¨ (æ•´ä¸ªä»»åŠ¡åªå¯åŠ¨è¿™ä¸€æ¬¡)
        await fetcher.start()

        # 3. å¼€å§‹é€’å½’é€»è¾‘
        final_data = await _recursive_crawl_logic(
            url, 
            pipelines=crawl_scopes, 
            current_depth=0, 
            max_items=max_items, 
            visited_urls=visited_urls,
            fetcher=fetcher, # æ³¨å…¥ fetcher
            max_pages=max_pages,
            max_scrolls=max_scrolls,
            wait_time=wait_time,
            max_nodes=max_nodes
        )
    except Exception as e:
        print(f"âŒ Critical Error during crawl: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # 4. ä»»åŠ¡ç»“æŸï¼Œå…³é—­æµè§ˆå™¨
        await fetcher.stop()

    return {
        "root_url": url,
        "depth_configured": len(crawl_scopes),
        "data": final_data
    }

# ==========================================
# 5. åŒæ­¥åŒ…è£…å™¨ (Sync Wrappers)
# ==========================================

def _run_async(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        return loop.run_until_complete(coro)
    else:
        return asyncio.run(coro)
    
# æ·»åŠ åˆ° agent/tools/crawl_tool.py æœ«å°¾

def sync_playwright_fetch(url: str, target: List[str], max_scrolls: int = 0, headless: bool = False, wait_time: int = 1000, max_nodes: int = 200) -> Dict:
    """
    [åŒæ­¥åŒ…è£…å™¨] åŸºç¡€å•é¡µé¢æŠ“å– (å¤ç”¨ PersistentFetcher)
    """
    async def _runner():
        # ä¸ºäº†å•æ¬¡è°ƒç”¨ä¹Ÿäº«å—æŒä¹…åŒ–ï¼Œæˆ‘ä»¬è¿™é‡Œä¸´æ—¶å®ä¾‹åŒ–ä¸€ä¸ª fetcher
        fetcher = PersistentFetcher(headless=headless)
        try:
            await fetcher.start()
            return await fetcher.fetch(url, target, max_scrolls=max_scrolls, wait_time=wait_time, max_nodes=max_nodes)
        finally:
            await fetcher.stop()
            
    return _run_async(_runner())

def sync_hierarchical_crawl(
    url: str, 
    crawl_scopes: List[List[str]], 
    max_items: int = 3, 
    max_pages: int = 3, 
    max_scrolls: int = 1,
    headless: bool = False,
    wait_time: int = 1000,
    max_nodes: int = 200
) -> Dict:
    """
    [æ–°ç‰ˆ] å¤šå±‚çº§çˆ¬è™«åŒæ­¥å…¥å£
    """
    return _run_async(hierarchical_crawl(url, crawl_scopes, max_items, max_pages, max_scrolls, headless, wait_time, max_nodes))

# ä½¿ç”¨ç¤ºä¾‹ (å¯é€‰)
if __name__ == "__main__":
    # ç¤ºä¾‹é…ç½®
    start_url = "https://example.com/list"
    scopes = [
        ["ç”µå½±åç§°", "è¯„åˆ†"],         # ç¬¬ä¸€å±‚: åˆ—è¡¨é¡µ
        ["å‰§æƒ…ç®€ä»‹", "ä¸‹è½½åœ°å€"]      # ç¬¬äºŒå±‚: è¯¦æƒ…é¡µ
    ]
    
    # è¿è¡Œ
    # result = sync_hierarchical_crawl(start_url, scopes, max_items=2, headless=False)
    # print(result)
    pass