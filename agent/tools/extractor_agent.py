import os
import re
import html
import json
from typing import List, Dict, Any, Set, Union
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from agent.prompt_template import SCRAWL_DATA_SYSTEM_PROMPT

load_dotenv()

MODA_OPENAI_API_KEY = os.environ.get("MODA_OPENAI_API_KEY")
MODA_OPENAI_BASE_URL = os.environ.get("MODA_OPENAI_BASE_URL")
MODEL = os.environ.get("MODA_MODEL_NAME", "gpt-4o-mini")

class ExtractorAgent:
    def __init__(self):
        self.llm = ChatOpenAI(
            model=MODEL, 
            temperature=0.1, # é™ä½æ¸©åº¦ä»¥æé«˜æ ¼å¼ç¨³å®šæ€§
            openai_api_key=MODA_OPENAI_API_KEY, 
            openai_api_base=MODA_OPENAI_BASE_URL
        )

    def get_content(self, fetched_html: str, target: List[str], source: str) -> Any:
        """
        æ ¹æ® HTML å’Œç›®æ ‡å­—æ®µï¼Œä½¿ç”¨ LLM æå–ç»“æ„åŒ–æ•°æ®
        """
        # 1. é¢„æ£€æŸ¥ï¼šå¦‚æœ HTML å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œç›´æ¥è·³è¿‡
        if not fetched_html or len(fetched_html.strip()) < 10:
            print("âš ï¸ è­¦å‘Š: è¾“å…¥çš„ HTML å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œè·³è¿‡æå–ã€‚")
            return []

        # ============================================================
        # ã€æ ¸å¿ƒä¿®å¤ã€‘ä¸¥æ ¼çš„é•¿åº¦æˆªæ–­æœºåˆ¶ï¼Œé¿å… 400 Input Length Error
        # ============================================================
        # è®¾å®šå®‰å…¨é˜ˆå€¼ã€‚æŠ¥é”™æç¤ºä¸Šé™æ˜¯ 129024ï¼Œæˆ‘ä»¬ç•™å‡º 20k ç»™ System Prompt å’Œ User Query
        # å®é™…ä¸Š 100k å­—ç¬¦é€šå¸¸èƒ½è¦†ç›–ç»å¤§å¤šæ•°ç½‘é¡µçš„æ ¸å¿ƒå†…å®¹
        MAX_INPUT_LENGTH = 256000
        
        if len(fetched_html) > MAX_INPUT_LENGTH:
            print(f"âš ï¸ è¾“å…¥å†…å®¹è¿‡é•¿ ({len(fetched_html)} chars)ï¼Œæ­£åœ¨æˆªæ–­è‡³ {MAX_INPUT_LENGTH} å­—ç¬¦ä»¥é¿å…æŠ¥é”™...")
            # æˆªæ–­å¹¶æ·»åŠ æ ‡è®°ï¼Œè®© LLM çŸ¥é“å†…å®¹ä¸å®Œæ•´
            fetched_html = fetched_html[:MAX_INPUT_LENGTH] + "\n\n...(Content Truncated due to length limit)..."

        # 2. å‘é€ç»™ LLM
        prompt = PromptTemplate.from_template(SCRAWL_DATA_SYSTEM_PROMPT)
        
        try:
            # å°†åˆ—è¡¨ç±»å‹çš„ target è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿ Prompt ç†è§£
            resp = self.llm.invoke(prompt.format(user_query=str(target), summary=fetched_html, source=source))
            content = resp.content.strip()
        except Exception as e:
            # æ•è·æ‰€æœ‰ LLM è°ƒç”¨å¼‚å¸¸ï¼Œé˜²æ­¢æ•´ä¸ª Agent å´©æºƒ
            error_str = str(e)
            print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {error_str}")
            
            # å¦‚æœæˆªæ–­åä¾ç„¶æŠ¥é”™ï¼ˆæå°‘æ•°æƒ…å†µï¼‰ï¼Œè¿”å›å‹å¥½çš„é”™è¯¯ç»“æ„
            if "400" in error_str or "length" in error_str.lower():
                return {"error": "Content too long for LLM", "details": "Please try reducing the crawl scope or target."}
            
            return {"error": "LLM invocation failed", "details": error_str}

        # 3. è§£æ JSON (åŒ…å«é‡è¯•ã€æ­£åˆ™æå–å’Œæ¸…æ´—é€»è¾‘)
        result = self._parse_json_safely(content)

        # 4. ç»“æœå»é‡ (ä»…é’ˆå¯¹åˆ—è¡¨ç»“æœ)
        if isinstance(result, list):
            return self._deduplicate_items(result)
        
        # å¦‚æœè§£æå‡ºé”™è¿”å›äº†å­—å…¸å½¢å¼çš„é”™è¯¯ä¿¡æ¯
        if isinstance(result, dict) and "error" in result:
            print(f"âš ï¸ æå–å¤±è´¥: {result['error']}")
            # åœ¨å¤šå±‚çˆ¬å–ä¸­ï¼Œæå–å¤±è´¥æœ€å¥½è¿”å›ç©ºåˆ—è¡¨ï¼Œä»¥å…æ‰“æ–­é€’å½’
            return []

        return result

    def _parse_json_safely(self, text: str) -> Union[List, Dict]:
        """
        å®‰å…¨åœ°è§£æ JSONï¼ŒåŒ…å«å¤šå±‚æ¸…æ´—ç­–ç•¥
        """
        # ç­–ç•¥ 1: ç›´æ¥è§£æ
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # ç­–ç•¥ 2: å»é™¤ Markdown ä»£ç å—æ ‡è®°
        cleaned_text = text.replace("```json", "").replace("```", "").strip()
        try:
            return json.loads(cleaned_text)
        except json.JSONDecodeError:
            pass

        # ç­–ç•¥ 3: æ­£åˆ™è¡¨è¾¾å¼æå– [ ... ]
        # ä¸“é—¨åº”å¯¹ LLM åœ¨ JSON å‰ååŠ åºŸè¯çš„æƒ…å†µ (ä¾‹å¦‚: "Here is the json: [...]")
        try:
            # re.DOTALL è®© . åŒ¹é…æ¢è¡Œç¬¦
            match = re.search(r'\[.*\]', text, re.DOTALL)
            if match:
                potential_json = match.group(0)
                return json.loads(potential_json)
        except json.JSONDecodeError:
            pass
            
        # ç­–ç•¥ 4: å°è¯•æå–å¯¹è±¡ { ... } (å¦‚æœè¿”å›çš„æ˜¯å•ä¸ªå¯¹è±¡è€Œéåˆ—è¡¨)
        try:
            match = re.search(r'\{.*\}', text, re.DOTALL)
            if match:
                potential_json = match.group(0)
                return json.loads(potential_json)
        except json.JSONDecodeError:
            pass

        # ç­–ç•¥ 5: å½»åº•å¤±è´¥
        print(f"âŒ JSON è§£æå½»åº•å¤±è´¥ã€‚åŸå§‹å†…å®¹é¢„è§ˆ: {text[:100]}...")
        return {"error": "Failed to parse JSON", "raw_content": text}

    def _deduplicate_items(self, items: List[Dict]) -> List[Dict]:
        """
        å¯¹æå–ç»“æœåˆ—è¡¨è¿›è¡Œæ™ºèƒ½å»é‡
        ç­–ç•¥ï¼šè‡ªåŠ¨æŸ¥æ‰¾ url/link å­—æ®µï¼Œå¦‚æœ URL ç›¸åŒåˆ™è§†ä¸ºé‡å¤æ¡ç›®ã€‚
        """
        if not items:
            return []

        unique_items = []
        seen_urls = set()
        
        # å¯èƒ½è¡¨ç¤ºé“¾æ¥çš„é”®å
        target_keys = ["url", "link", "href", "é“¾æ¥", "è¯¦æƒ…é¡µé“¾æ¥", "æ–‡ç« é“¾æ¥", "ç”µå½±é“¾æ¥", "source", "detail_url", "full_url"]

        for item in items:
            if not isinstance(item, dict):
                unique_items.append(item)
                continue

            found_url = None
            # æ™ºèƒ½å¯»æ‰¾è¯¥æ¡ç›®ä¸­çš„ URL å€¼
            for k, v in item.items():
                if k.lower() in target_keys and v and isinstance(v, str):
                    found_url = v.strip()
                    break
            
            if found_url:
                # æ ‡å‡†åŒ– URL (å»é™¤æœ«å°¾æ–œæ )
                normalized_url = found_url.rstrip('/')
                
                if normalized_url in seen_urls:
                    continue # è·³è¿‡é‡å¤
                
                seen_urls.add(normalized_url)
                unique_items.append(item)
            else:
                # å¦‚æœæ²¡æ‰¾åˆ° URL å­—æ®µï¼Œé»˜è®¤ä¿ç•™
                unique_items.append(item)

        if len(items) != len(unique_items):
            print(f"ğŸ” ExtractorAgent å»é‡ä¼˜åŒ–: {len(items)} -> {len(unique_items)} æ¡")
        
        return unique_items