import json
import re
import time
import traceback
from typing import List, Dict, Any, Union, Optional
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document 
from langchain_core.prompts import PromptTemplate
from langchain_community.document_transformers import Html2TextTransformer

# å¼•å…¥æ ¸å¿ƒ DOM å·¥å…· (ä» agent.tools.dom_helper)
from agent.tools.dom_helper import dom_analyzer
# å¼•å…¥æç¤ºè¯
from agent.prompt_template import XPATH_ANALYSIS_PROMPT, SCRAWL_DATA_SYSTEM_PROMPT
from config import *

load_dotenv()

class ExtractorAgent:
    def __init__(self):
        self.llm = ChatOpenAI(
            model=MODEL_NAME, 
            temperature=0, # å¿…é¡»ä¸º 0ï¼Œä¿è¯é€»è¾‘ç²¾å‡†
            openai_api_key=OPENAI_API_KEY, 
            openai_api_base=OPENAI_BASE_URL
        )
        self.html2text = Html2TextTransformer(ignore_links=False)
    def _sanitize_for_llm(self, text: str, aggressive: bool = False) -> str:
        """
        ã€å®‰å…¨æ¸…æ´—ã€‘åœ¨å‘é€ç»™ LLM å‰æ¸…æ´—æ–‡æœ¬ã€‚
        aggressive=True æ—¶å¯ç”¨å¼ºåŠ›æ¨¡å¼ï¼Œç”¨äºé‡è¯•ã€‚
        """
        if not text: return ""
        
        # 1. ç§»é™¤æ§åˆ¶å­—ç¬¦
        text = re.sub(r'[\x00-\x09\x0b\x0c\x0e-\x1f\x7f]', '', text)
        
        # 2. ç§»é™¤ Base64 å›¾ç‰‡
        text = re.sub(r'data:image\/[a-zA-Z]+;base64,[a-zA-Z0-9+/=]+', '[BASE64_REMOVED]', text)
        
        # 3. ç§»é™¤è¶…é•¿å­—ç¬¦ä¸² (å¦‚åŠ å¯† Token, CSS chunks)
        text = re.sub(r'[a-zA-Z0-9+/=]{80,}', '[LONG_TOKEN_REMOVED]', text)
        
        if aggressive:
            # ã€å¼ºåŠ›æ¨¡å¼ã€‘
            # 1. ç§»é™¤ URL å‚æ•° (å¾€å¾€åŒ…å«æ•æ„Ÿè¿½è¸ª ID)
            # åŒ¹é… http... ?a=b... æ›¿æ¢ä¸º [URL_PARAM_REMOVED]
            text = re.sub(r'(https?://[^?\s]+)\?[^\s]*', r'\1?[PARAMS_REMOVED]', text)
            
            # 2. ç§»é™¤æ‰€æœ‰è„šæœ¬/æ ·å¼é—ç•™ (ä»¥é˜²ä¸‡ä¸€)
            text = re.sub(r'<script.*?>.*?</script>', '[SCRIPT_REMOVED]', text, flags=re.DOTALL)
            text = re.sub(r'<style.*?>.*?</style>', '[STYLE_REMOVED]', text, flags=re.DOTALL)
            
            # 3. è¿‡æ»¤æ‰é ASCII ä¸”éä¸­æ—¥éŸ©å­—ç¬¦çš„æ€ªå¼‚ç¬¦å· (Emoji é™¤å¤–)
            # è¿™é‡Œç®€å•å¤„ç†ï¼šå¦‚æœä¸€è¡Œé‡Œä¹±ç å¤ªå¤šï¼Œç›´æ¥ä¸¢å¼ƒè¯¥è¡Œ? 
            # æš‚æ—¶åªåš URL æ¸…æ´—ï¼Œé€šå¸¸è¿™å°±å¤Ÿäº†ã€‚
            
        return text

    def get_content(self, fetched_html: str, target: List[str], source: str, max_nodes: int = 200) -> Dict[str, Any]:
        """
        æ•°æ®æå–ä¸»å…¥å£
        """
        # 1. é¢„æ£€æŸ¥
        if not fetched_html or len(fetched_html.strip()) < 10:
            print("âš ï¸ è­¦å‘Š: è¾“å…¥çš„ HTML å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œè·³è¿‡æå–ã€‚")
            return {"items": [], "next_page_url": None}

        print(f"ğŸ—ï¸ [Extractor] å¼€å§‹å¤„ç† URL: {source}")
        
        # ============================================================
        # ç­–ç•¥ A: éª¨æ¶åˆ†ææ³• (ä¼˜å…ˆ)
        # ============================================================
        try:
            # 1. ç”Ÿæˆéª¨æ¶
            html_snippet = fetched_html[:80000]
            skeleton = dom_analyzer.summarize_structure(html_snippet, max_nodes=max_nodes)

            # 2. åˆæ¬¡å°è¯• (æ ‡å‡†æ¸…æ´—)
            safe_skeleton = self._sanitize_for_llm(skeleton, aggressive=False)
            
            if len(safe_skeleton) > 100:
                print(f"ğŸ¦´ DOM éª¨æ¶ç”Ÿæˆå®Œæ¯• ({len(safe_skeleton)} chars)ã€‚è¯·æ±‚ LLM ç”Ÿæˆ XPath...")
                
                prompt = PromptTemplate.from_template(XPATH_ANALYSIS_PROMPT)
                user_query_str = f"æå–å­—æ®µ: {', '.join(target)}"
                
                try:
                    resp = self.llm.invoke(prompt.format(user_query=user_query_str, skeleton=safe_skeleton))
                    json_str = resp.content
                except Exception as e:
                    # æ•æ‰ 400 é£æ§é”™è¯¯
                    error_str = str(e)
                    if "data_inspection_failed" in error_str or "400" in error_str:
                        print("âš ï¸ è§¦å‘å†…å®¹é£æ§ (Level 1)ï¼Œå°è¯•å¼ºåŠ›æ¸…æ´—é‡è¯•...")
                        # 3. é‡è¯•æœºåˆ¶ (å¼ºåŠ›æ¸…æ´—)
                        safe_skeleton_aggressive = self._sanitize_for_llm(skeleton, aggressive=True)
                        try:
                            # ç­‰å¾… 1 ç§’å†é‡è¯•ï¼Œé¿å…å¹¶å‘é™åˆ¶
                            time.sleep(1)
                            resp = self.llm.invoke(prompt.format(user_query=user_query_str, skeleton=safe_skeleton_aggressive))
                            json_str = resp.content
                            print("âœ… å¼ºåŠ›æ¸…æ´—å LLM è¯·æ±‚æˆåŠŸï¼")
                        except Exception as e2:
                            print(f"âŒ å¼ºåŠ›æ¸…æ´—åä¾ç„¶å¤±è´¥: {e2}")
                            raise e2 # æŠ›å‡ºå¼‚å¸¸ï¼Œè§¦å‘å›é€€
                    else:
                        raise e # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º

                # å¤„ç† LLM è¿”å›
                clean_json = json_str.strip().replace("```json", "").replace("```", "")
                try:
                    rule = json.loads(clean_json)
                    print(f"ğŸ¯ LLM ç”Ÿæˆ XPath è§„åˆ™: {json.dumps(rule, ensure_ascii=False)}")
                    
                    extracted_items = dom_analyzer.extract_by_xpath(fetched_html, rule)
                    
                    if extracted_items and len(extracted_items) > 0:
                        print(f"âœ… XPath æå–æˆåŠŸ! å…± {len(extracted_items)} æ¡æ•°æ®")
                        next_url = self._try_extract_next_page_by_regex(fetched_html)
                        if next_url:
                            print(f"      ğŸ” [Regex] è¡¥å……æå–åˆ°ç¿»é¡µé“¾æ¥: {next_url}")
                        return {
                            "items": extracted_items,
                            "next_page_url": next_url
                        }
                    else:
                        print("âš ï¸ XPath è§„åˆ™æ‰§è¡Œç»“æœä¸ºç©ºï¼Œå°è¯•å›é€€...")
                except json.JSONDecodeError:
                    print(f"âš ï¸ XPath è§„åˆ™è§£æå¤±è´¥: {resp.content}")
            else:
                print("âš ï¸ éª¨æ¶ç”Ÿæˆè¿‡çŸ­ï¼Œè·³è¿‡ XPath ç­–ç•¥ã€‚")
                
        except Exception as e:
            print(f"âš ï¸ XPath ç­–ç•¥æœ€ç»ˆæ‰§è¡Œå¼‚å¸¸ (å·²å›é€€): {e}")
            # traceback.print_exc() 

        # ============================================================
        # ç­–ç•¥ B: æ–‡æœ¬åˆ†å—æå– (å…œåº•)
        # ============================================================
        print("ğŸ”„ å›é€€åˆ°çº¯æ–‡æœ¬ LLM åˆ†å—æå–æ¨¡å¼...")
        # è¿™é‡Œçš„è¾“å…¥ html ä¹Ÿè¦æ³¨æ„ï¼Œå¿…é¡»ä½¿ç”¨å¼ºåŠ›æ¸…æ´—ï¼Œå¦åˆ™åˆ†å—ä¹Ÿä¼šæŒ‚
        safe_html = self._sanitize_for_llm(fetched_html, aggressive=True)
        return self._extract_by_chunking_strategy(safe_html, target, source)

    # ============================================================
    # è¾…åŠ©æ–¹æ³• & å…œåº•é€»è¾‘
    # ============================================================
    
    def _extract_by_chunking_strategy(self, fetched_html: str, target: List[str], source: str) -> Dict[str, Any]:
        """
        åŸæœ‰çš„åˆ†å—æå–é€»è¾‘ (Slow Path)
        """
        CHUNK_SIZE = 20000 
        if len(fetched_html) <= CHUNK_SIZE:
            # ã€å®‰å…¨ä¿®å¤ã€‘åˆ†å—æ¨¡å¼ä¹Ÿéœ€è¦æ¸…æ´—
            safe_html = self._sanitize_for_llm(fetched_html)
            return self._process_single_chunk(safe_html, target, source)
        
        docs = [Document(page_content=safe_html, metadata={"source": source})]
        transformed_docs = self.html2text.transform_documents(docs)
        pure_text = transformed_docs[0].page_content if transformed_docs else ""

        print(f"ğŸ“¦ å†…å®¹è¿‡é•¿ ({len(fetched_html)} chars)ï¼Œå¯åŠ¨åˆ†å—æå–...")
        chunks = self._split_text_by_lines(pure_text, CHUNK_SIZE)
        all_items = []
        detected_next_page = None
        
        for i, chunk in enumerate(chunks):
            # ã€å®‰å…¨ä¿®å¤ã€‘æ¸…æ´—æ¯ä¸€ä¸ªå—
            print(f"ğŸ“¦ åˆ†å— {i+1}/{len(chunks)}: {len(chunk)} chars")
            safe_chunk = self._sanitize_for_llm(chunk)
            chunk_result = self._process_single_chunk(safe_chunk, target, source)
            items = chunk_result.get("items", [])
            if items: all_items.extend(items)
            if chunk_result.get("next_page_url"): detected_next_page = chunk_result["next_page_url"]

        return {
            "items": self._deduplicate_items(all_items),
            "next_page_url": detected_next_page
        }

    def _process_single_chunk(self, chunk_text: str, target: List[str], source: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡æœ¬å—"""
        prompt = PromptTemplate.from_template(SCRAWL_DATA_SYSTEM_PROMPT)
        try:
            resp = self.llm.invoke(prompt.format(user_query=str(target), summary=chunk_text, source=source))
            content = resp.content.strip()
        except Exception as e:
            return {"items": [], "next_page_url": None}

        raw_result = self._parse_json_safely(content)
        final_structure = {"items": [], "next_page_url": None}

        if isinstance(raw_result, dict):
            if "items" in raw_result:
                final_structure["items"] = raw_result["items"] if isinstance(raw_result["items"], list) else []
                final_structure["next_page_url"] = raw_result.get("next_page_url")
            elif "items" not in raw_result: 
                 final_structure["items"] = [raw_result]
        elif isinstance(raw_result, list):
            final_structure["items"] = raw_result
        
        if not final_structure.get("next_page_url"):
            fallback_url = self._try_extract_next_page_by_regex(chunk_text)
            if fallback_url: final_structure["next_page_url"] = fallback_url

        return final_structure

    def _try_extract_next_page_by_regex(self, text: str) -> Union[str, None]:
        """æ­£åˆ™å…œåº•æå–ç¿»é¡µé“¾æ¥"""
        keywords = r"(æ›´å¤š|Next|ä¸‹ä¸€é¡µ|ä¸‹é¡µ|More|>>|Â»)"
        pattern = re.compile(r'\[\s*([^\]]*?' + keywords + r'[^\]]*?)\s*\]\((https?://[^)]+)\)', re.IGNORECASE)
        html_pattern = re.compile(r'<a[^>]*href=["\']([^"\']+)["\'][^>]*>.*?'+keywords+'.*?</a>', re.IGNORECASE)
        
        matches = pattern.findall(text)
        if matches: return matches[0][2]
        
        html_matches = html_pattern.findall(text)
        if html_matches: return html_matches[0]
        return None

    def _split_text_by_lines(self, text: str, max_length: int) -> List[str]:
        lines = text.split('\n')
        chunks = []
        current_chunk = []
        current_length = 0
        for line in lines:
            line_len = len(line) + 1 
            if line_len > max_length:
                if current_chunk:
                    chunks.append("\n".join(current_chunk))
                    current_chunk = []
                    current_length = 0
                while len(line) > max_length:
                    chunks.append(line[:max_length])
                    line = line[max_length:]
                current_chunk = [line]
                current_length = len(line) + 1
                continue 
            if current_length + line_len > max_length:
                chunks.append("\n".join(current_chunk))
                current_chunk = [line]
                current_length = line_len
            else:
                current_chunk.append(line)
                current_length += line_len
        if current_chunk: chunks.append("\n".join(current_chunk))
        return chunks

    def _parse_json_safely(self, text: str) -> Union[List, Dict]:
        try: return json.loads(text)
        except: pass
        cleaned = text.replace("```json", "").replace("```", "").strip()
        try: return json.loads(cleaned)
        except: pass
        try:
            match = re.search(r'\{[\s\S]*\}', text) 
            if match: return json.loads(match.group(0))
        except: pass
        try:
            match = re.search(r'\[[\s\S]*\]', text)
            if match: return json.loads(match.group(0))
        except: pass
        return {"items": [], "next_page_url": None}

    def _deduplicate_items(self, items: List[Dict]) -> List[Dict]:
        if not items: return []
        unique_items = []
        seen_urls = set()
        target_keys = ["url", "link", "href", "é“¾æ¥"]
        for item in items:
            if not isinstance(item, dict):
                unique_items.append(item)
                continue
            found_url = None
            for k, v in item.items():
                if k.lower() in target_keys and v and isinstance(v, str):
                    found_url = v.strip()
                    break
            if found_url:
                normalized = found_url.rstrip('/')
                if normalized in seen_urls: continue
                seen_urls.add(normalized)
                unique_items.append(item)
            else:
                unique_items.append(item)
        return unique_items