import os
import re
import html
import json
from typing import List, Dict, Any, Set, Union
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
# ä¿®å¤å¯¼å…¥è·¯å¾„ï¼Œé¿å… ImportError
from langchain_core.prompts import PromptTemplate
from agent.prompt_template import SCRAWL_DATA_SYSTEM_PROMPT
from config import *

load_dotenv()

# MODA_OPENAI_API_KEY = os.environ.get("MODA_OPENAI_API_KEY")
# MODA_OPENAI_BASE_URL = os.environ.get("MODA_OPENAI_BASE_URL")
# MODEL_NAME = os.environ.get("MODA_MODEL_NAME", "gpt-4o-mini")

class ExtractorAgent:
    def __init__(self):
        self.llm = ChatOpenAI(
            model=MODEL_NAME, 
            temperature=0.1, # é™ä½æ¸©åº¦ä»¥æé«˜æ ¼å¼ç¨³å®šæ€§
            openai_api_key=OPENAI_API_KEY, 
            openai_api_base=OPENAI_BASE_URL
        )

    def get_content(self, fetched_html: str, target: List[str], source: str) -> Dict[str, Any]:
        """
        æ ¹æ® HTML å’Œç›®æ ‡å­—æ®µï¼Œä½¿ç”¨ LLM æå–ç»“æ„åŒ–æ•°æ®
        Return: {"items": List[Dict], "next_page_url": str | None}
        """
        # 1. é¢„æ£€æŸ¥
        if not fetched_html or len(fetched_html.strip()) < 10:
            print("âš ï¸ è­¦å‘Š: è¾“å…¥çš„ HTML å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œè·³è¿‡æå–ã€‚")
            return {"items": [], "next_page_url": None}

        # ============================================================
        # åˆ†å—ç­–ç•¥ (Map-Reduce)
        # ============================================================
        CHUNK_SIZE = 20000  # 20k å­—ç¬¦å®‰å…¨é˜ˆå€¼
        
        # Fast Path: ä¸åˆ†å—
        if len(fetched_html) <= CHUNK_SIZE:
            return self._process_single_chunk(fetched_html, target, source)

        # Slow Path: åˆ†å—å¤„ç†
        print(f"ğŸ“¦ å†…å®¹è¿‡é•¿ ({len(fetched_html)} chars)ï¼Œå¯åŠ¨åˆ†å—æå– (Chunk Size: {CHUNK_SIZE})...")
        chunks = self._split_text_by_lines(fetched_html, CHUNK_SIZE)
        print(f"   -> åˆ‡åˆ†ä¸º {len(chunks)} å—ï¼Œå¼€å§‹é€å—æå–...")

        all_items = []
        detected_next_page = None
        
        for i, chunk in enumerate(chunks):
            # æå–å½“å‰å—
            chunk_result = self._process_single_chunk(chunk, target, source)
            
            # 1. æ”¶é›† items
            items = chunk_result.get("items", [])
            if items:
                all_items.extend(items)
                print(f"âœ… ç¬¬ {i+1} å—æå–åˆ° {len(items)} æ¡æ•°æ®")
            
            # 2. æ”¶é›† next_page_url 
            # ç¿»é¡µé“¾æ¥é€šå¸¸åœ¨é¡µé¢çš„åº•éƒ¨ï¼ˆå³æœ€åå‡ ä¸ªå—ä¸­ï¼‰
            # å¦‚æœåé¢çš„å—å‘ç°äº†ç¿»é¡µé“¾æ¥ï¼Œè¦†ç›–ä¹‹å‰çš„
            if chunk_result.get("next_page_url"):
                detected_next_page = chunk_result["next_page_url"]
                print(f"      ğŸ” ç¬¬ {i+1} å—å‘ç°äº†ç¿»é¡µé“¾æ¥: {detected_next_page}")

        print(f"ğŸ“¦ åˆ†å—æå–å®Œæˆï¼ŒåŸå§‹æ€»æ¡æ•°: {len(all_items)}")

        # å…¨å±€å»é‡
        final_items = self._deduplicate_items(all_items)
        
        return {
            "items": final_items,
            "next_page_url": detected_next_page
        }

    def _process_single_chunk(self, chunk_text: str, target: List[str], source: str) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªå—ï¼Œè¿”å› {"items": [], "next_page_url": ...}
        """
        prompt = PromptTemplate.from_template(SCRAWL_DATA_SYSTEM_PROMPT)
        
        try:
            # user_query è½¬å­—ç¬¦ä¸²ï¼Œé¿å…ç”±åˆ—è¡¨å¼•å‘æ ¼å¼é—®é¢˜
            resp = self.llm.invoke(prompt.format(user_query=str(target), summary=chunk_text, source=source))
            content = resp.content.strip()
        except Exception as e:
            print(f"âŒ LLM Chunk Error: {e}")
            return {"items": [], "next_page_url": None}

        # è§£æ JSON
        raw_result = self._parse_json_safely(content)
        
        # æ ¼å¼æ ‡å‡†åŒ–ï¼šç¡®ä¿è¿”å›ç»“æ„æ˜¯ {"items": [], "next_page_url": None}
        final_structure = {"items": [], "next_page_url": None}

        if isinstance(raw_result, dict):
            # æƒ…å†µ A: æ ‡å‡†è¿”å› {"items": [...], "next_page_url": "..."}
            if "items" in raw_result:
                final_structure["items"] = raw_result["items"] if isinstance(raw_result["items"], list) else []
                final_structure["next_page_url"] = raw_result.get("next_page_url")
            # æƒ…å†µ B: LLM è¿˜æ˜¯è¿”å›äº†æ—§æ ¼å¼çš„å•ä¸ªå¯¹è±¡ (è™½ç„¶ Prompt ç¦æ­¢äº†)
            elif "items" not in raw_result: 
                 # å°è¯•æŠŠæ•´ä¸ª dict å½“ä½œä¸€ä¸ª itemï¼Œæ’é™¤ error å­—æ®µçš„æƒ…å†µ
                 if "error" not in raw_result:
                     final_structure["items"] = [raw_result]

        elif isinstance(raw_result, list):
            # æƒ…å†µ C: LLM è¿”å›äº†çº¯åˆ—è¡¨ (æ—§æ ¼å¼)
            final_structure["items"] = raw_result
        
        return final_structure

    def _split_text_by_lines(self, text: str, max_length: int) -> List[str]:
        """æŒ‰è¡Œåˆ‡åˆ†æ–‡æœ¬ï¼Œå¹¶å®‰å…¨å¤„ç†è¶…é•¿è¡Œ"""
        lines = text.split('\n')
        chunks = []
        current_chunk = []
        current_length = 0
        
        for line in lines:
            line_len = len(line) + 1 # +1 æ˜¯è€ƒè™‘æ¢è¡Œç¬¦
            
            # --- ä¿®å¤å¼€å§‹ï¼šå¤„ç†å•è¡Œè¶…é•¿çš„æƒ…å†µ ---
            if line_len > max_length:
                # 1. å…ˆæŠŠæ‰‹å¤´ç§¯æ”’çš„ current_chunk å­˜æ‰
                if current_chunk:
                    chunks.append("\n".join(current_chunk))
                    current_chunk = []
                    current_length = 0
                
                # 2. å¾ªç¯åˆ‡åˆ†å½“å‰è¿™è¡Œè¶…é•¿çš„æ–‡æœ¬
                # æ¯”å¦‚ line æœ‰ 70kï¼Œmax_length æ˜¯ 30kï¼Œè¿™é‡Œä¼šåˆ‡æˆ 30k, 30k, 10k
                while len(line) > max_length:
                    # åˆ‡ä¸‹å‰ max_length ä¸ªå­—ç¬¦ä½œä¸ºä¸€ä¸ªå•ç‹¬çš„ chunk
                    chunks.append(line[:max_length])
                    # æŠŠå‰©ä¸‹çš„éƒ¨åˆ†èµ‹å€¼å› lineï¼Œç»§ç»­å¤„ç†
                    line = line[max_length:]
                
                # 3. å‰©ä¸‹çš„éƒ¨åˆ†ï¼ˆä¹Ÿå°±æ˜¯ < max_length çš„éƒ¨åˆ†ï¼‰ä¸èƒ½ä¸¢
                # æŠŠå®ƒä½œä¸ºæ–° chunk çš„å¼€å¤´ï¼Œæ”¾å…¥ current_chunk
                current_chunk = [line]
                current_length = len(line) + 1
                continue 
            # --- ä¿®å¤ç»“æŸ ---

            # ä¸‹é¢æ˜¯æ­£å¸¸è¡Œçš„å¤„ç†é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
            if current_length + line_len > max_length:
                chunks.append("\n".join(current_chunk))
                current_chunk = [line]
                current_length = line_len
            else:
                current_chunk.append(line)
                current_length += line_len
        
        # å¤„ç†æœ€åå‰©ä¸‹çš„ residue
        if current_chunk:
            chunks.append("\n".join(current_chunk))
            
        return chunks

    def _parse_json_safely(self, text: str) -> Union[List, Dict]:
        """å®‰å…¨è§£æ JSON"""
        # 1. å°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(text)
        except:
            pass

        # 2. æ¸…æ´— Markdown ä»£ç å—æ ‡è®°
        cleaned = text.replace("```json", "").replace("```", "").strip()
        try:
            return json.loads(cleaned)
        except:
            pass

        # 3. æ­£åˆ™æå–ï¼šä¼˜å…ˆå°è¯•æå–å¯¹è±¡ç»“æ„ {...} (æ–° Prompt è¦æ±‚è¿”å›å¯¹è±¡)
        try:
            # dotall æ¨¡å¼ï¼Œè®© . åŒ¹é…æ¢è¡Œç¬¦
            match = re.search(r'\{[\s\S]*\}', text) 
            if match:
                return json.loads(match.group(0))
        except:
            pass

        # 4. æ­£åˆ™æå–ï¼šå…œåº•å°è¯•æå–æ•°ç»„ [...] (é˜²æ­¢ LLM è¿”å›æ—§æ ¼å¼)
        try:
            match = re.search(r'\[[\s\S]*\]', text)
            if match:
                return json.loads(match.group(0))
        except:
            pass

        return {"items": [], "next_page_url": None, "error": "Parse Failed"}

    def _deduplicate_items(self, items: List[Dict]) -> List[Dict]:
        """ç»“æœå»é‡"""
        if not items: return []
        unique_items = []
        seen_urls = set()
        target_keys = ["url", "link", "href", "é“¾æ¥", "è¯¦æƒ…é¡µé“¾æ¥", "æ–‡ç« é“¾æ¥", "full_url"]

        for item in items:
            if not isinstance(item, dict):
                unique_items.append(item)
                continue
            
            found_url = None
            for k, v in item.items():
                if k.lower() in target_keys and v and isinstance(v, str):
                    found_url = v.strip()
                    break
            
            if found_url:
                normalized = found_url.rstrip('/')
                if normalized in seen_urls: continue
                seen_urls.add(normalized)
                unique_items.append(item)
            else:
                unique_items.append(item)
        
        if len(items) != len(unique_items):
            print(f"ğŸ” ExtractorAgent å…¨å±€å»é‡: {len(items)} -> {len(unique_items)} æ¡")
        return unique_items