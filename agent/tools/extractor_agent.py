import os
import re
import html
import json
import time
from typing import List, Dict, Any, Set, Union
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from agent.prompt_template import SCRAWL_DATA_SYSTEM_PROMPT

load_dotenv()

MODA_OPENAI_API_KEY = os.environ.get("MODA_OPENAI_API_KEY")
MODA_OPENAI_BASE_URL = os.environ.get("MODA_OPENAI_BASE_URL")
MODEL = os.environ.get("MODA_MODEL_NAME", "gpt-4o-mini")

class ExtractorAgent:
    def __init__(self):
        self.llm = ChatOpenAI(
            model=MODEL, 
            temperature=0.1, # é™ä½æ¸©åº¦ä»¥æé«˜æ ¼å¼ç¨³å®šæ€§
            openai_api_key=MODA_OPENAI_API_KEY, 
            openai_api_base=MODA_OPENAI_BASE_URL
        )

    def get_content(self, fetched_html: str, target: List[str], source: str) -> Any:
        """
        æ ¹æ® HTML å’Œç›®æ ‡å­—æ®µï¼Œä½¿ç”¨ LLM æå–ç»“æ„åŒ–æ•°æ®
        æ”¯æŒè‡ªåŠ¨åˆ†å—å¤„ç†è¶…é•¿æ–‡æœ¬ (Map-Reduce æ¨¡å¼)
        """
        # 1. é¢„æ£€æŸ¥
        if not fetched_html or len(fetched_html.strip()) < 10:
            print("âš ï¸ è­¦å‘Š: è¾“å…¥çš„ HTML å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œè·³è¿‡æå–ã€‚")
            return []

        # ============================================================
        # ã€æ ¸å¿ƒå‡çº§ã€‘åˆ†å—å¤„ç†ç­–ç•¥ (Map-Reduce)
        # ============================================================
        # è®¾å®šå•å—æœ€å¤§å­—ç¬¦æ•°ã€‚
        # ä¸­æ–‡ç¯å¢ƒä¸‹ï¼Œå®‰å…¨é˜ˆå€¼å»ºè®®è®¾ä¸º 20,000 - 30,000 å­—ç¬¦ (çº¦ 60k-90k bytes < 129k limit)
        # ç•™å‡ºä½™é‡ç»™ Prompt
        CHUNK_SIZE = 30000
        
        # å¦‚æœå†…å®¹æ€»é•¿åº¦å°äºé˜ˆå€¼ï¼Œç›´æ¥å¤„ç† (Fast Path)
        if len(fetched_html) <= CHUNK_SIZE:
            return self._process_single_chunk(fetched_html, target, source)

        # å¦‚æœå†…å®¹è¿‡é•¿ï¼Œè¿›è¡Œåˆ†å—
        print(f"ğŸ“¦ å†…å®¹è¿‡é•¿ ({len(fetched_html)} chars)ï¼Œå¯åŠ¨åˆ†å—æå–æ¨¡å¼ (Chunk Size: {CHUNK_SIZE})...")
        
        chunks = self._split_text_by_lines(fetched_html, CHUNK_SIZE)
        print(f"   -> åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—ï¼Œå¼€å§‹é€å—æå–...")

        all_results = []
        
        for i, chunk in enumerate(chunks):
            print(f"   ğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(chunks)} å— ({len(chunk)} chars)...")
            
            # æå–å½“å‰å—çš„æ•°æ®
            chunk_result = self._process_single_chunk(chunk, target, source)
            
            # æ”¶é›†ç»“æœ
            if isinstance(chunk_result, list):
                all_results.extend(chunk_result)
                print(f"      âœ… ç¬¬ {i+1} å—æå–åˆ° {len(chunk_result)} æ¡æ•°æ®")
            elif isinstance(chunk_result, dict) and "error" not in chunk_result:
                 # å¦‚æœ LLM è¿”å›äº†å•ä¸ªå¯¹è±¡ï¼ŒåŒ…ä¸€å±‚æ”¾è¿›å»
                all_results.append(chunk_result)
            
            # (å¯é€‰) ç®€å•çš„é€Ÿç‡é™åˆ¶ï¼Œé˜²æ­¢å¹¶å‘å¤ªå¿«è¢« API å°ç¦
            # time.sleep(0.5) 

        print(f"ğŸ“¦ åˆ†å—æå–å®Œæˆï¼ŒåŸå§‹æ€»æ¡æ•°: {len(all_results)}")

        # 4. å…¨å±€åˆå¹¶åå»é‡ (Reduce & Deduplicate)
        final_results = self._deduplicate_items(all_results)
        return final_results

    def _process_single_chunk(self, chunk_text: str, target: List[str], source: str) -> Any:
        """
        å†…éƒ¨æ–¹æ³•ï¼šå¤„ç†å•ä¸ªæ–‡æœ¬å—çš„æå–
        """
        prompt = PromptTemplate.from_template(SCRAWL_DATA_SYSTEM_PROMPT)
        
        try:
            resp = self.llm.invoke(prompt.format(user_query=str(target), summary=chunk_text, source=source))
            content = resp.content.strip()
        except Exception as e:
            error_str = str(e)
            print(f"âŒ LLM è°ƒç”¨å¤±è´¥ (Chunk): {error_str[:100]}...")
            return [] # å•å—å¤±è´¥ä¸å½±å“æ•´ä½“ï¼Œè¿”å›ç©ºåˆ—è¡¨

        # è§£æ JSON
        result = self._parse_json_safely(content)

        # ç®€å•çš„é”™è¯¯æ£€æŸ¥
        if isinstance(result, dict) and "error" in result:
            # print(f"âš ï¸ å—æå–è§£æå¤±è´¥: {result['error']}")
            return []
            
        return result

    def _split_text_by_lines(self, text: str, max_length: int) -> List[str]:
        """
        æŒ‰è¡Œå®‰å…¨åˆ‡åˆ†æ–‡æœ¬ï¼Œç¡®ä¿ä¸åˆ‡æ–­å®Œæ•´çš„è¡Œï¼ˆMarkdownç»“æ„ï¼‰
        """
        lines = text.split('\n')
        chunks = []
        current_chunk = []
        current_length = 0
        
        for line in lines:
            line_len = len(line) + 1 # +1 æ˜¯æ¢è¡Œç¬¦
            
            # å¦‚æœå•è¡Œæœ¬èº«å°±è¶…é•¿ï¼ˆæç«¯æƒ…å†µï¼‰ï¼Œå¼ºåˆ¶åˆ‡æ–­æˆ–å•ç‹¬æˆå—
            if line_len > max_length:
                # å¦‚æœå½“å‰å—æœ‰å†…å®¹ï¼Œå…ˆä¿å­˜
                if current_chunk:
                    chunks.append("\n".join(current_chunk))
                    current_chunk = []
                    current_length = 0
                # è¶…é•¿è¡Œå•ç‹¬ä½œä¸ºä¸€å—ï¼ˆæˆ–è€…ä½ å¯ä»¥é€‰æ‹©å¼ºåˆ¶æˆªæ–­ï¼Œè¿™é‡Œé€‰æ‹©ä¿ç•™ï¼‰
                chunks.append(line[:max_length]) 
                continue

            # å¦‚æœåŠ å…¥è¿™è¡Œä¼šè¶…é•¿ï¼Œåˆ™ä¿å­˜å½“å‰å—ï¼Œå¼€å¯æ–°å—
            if current_length + line_len > max_length:
                chunks.append("\n".join(current_chunk))
                current_chunk = [line]
                current_length = line_len
            else:
                current_chunk.append(line)
                current_length += line_len
        
        # ä¿å­˜æœ€åä¸€å—
        if current_chunk:
            chunks.append("\n".join(current_chunk))
            
        return chunks

    def _parse_json_safely(self, text: str) -> Union[List, Dict]:
        """
        å®‰å…¨åœ°è§£æ JSONï¼ŒåŒ…å«å¤šå±‚æ¸…æ´—ç­–ç•¥
        """
        # ç­–ç•¥ 1: ç›´æ¥è§£æ
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # ç­–ç•¥ 2: å»é™¤ Markdown ä»£ç å—æ ‡è®°
        cleaned_text = text.replace("```json", "").replace("```", "").strip()
        try:
            return json.loads(cleaned_text)
        except json.JSONDecodeError:
            pass

        # ç­–ç•¥ 3: æ­£åˆ™è¡¨è¾¾å¼æå– [ ... ]
        try:
            match = re.search(r'\[.*\]', text, re.DOTALL)
            if match:
                return json.loads(match.group(0))
        except json.JSONDecodeError:
            pass
            
        # ç­–ç•¥ 4: å°è¯•æå–å¯¹è±¡ { ... }
        try:
            match = re.search(r'\{.*\}', text, re.DOTALL)
            if match:
                return json.loads(match.group(0))
        except json.JSONDecodeError:
            pass

        # ç­–ç•¥ 5: å½»åº•å¤±è´¥
        # print(f"âŒ JSON è§£æå½»åº•å¤±è´¥ã€‚åŸå§‹å†…å®¹é¢„è§ˆ: {text[:50]}...")
        return {"error": "Failed to parse JSON", "raw_content": text}

    def _deduplicate_items(self, items: List[Dict]) -> List[Dict]:
        """
        å¯¹æå–ç»“æœåˆ—è¡¨è¿›è¡Œæ™ºèƒ½å»é‡
        """
        if not items:
            return []

        unique_items = []
        seen_urls = set()
        
        target_keys = ["url", "link", "href", "é“¾æ¥", "è¯¦æƒ…é¡µé“¾æ¥", "æ–‡ç« é“¾æ¥", "ç”µå½±é“¾æ¥", "source", "detail_url", "full_url"]

        for item in items:
            if not isinstance(item, dict):
                unique_items.append(item)
                continue

            found_url = None
            for k, v in item.items():
                if k.lower() in target_keys and v and isinstance(v, str):
                    found_url = v.strip()
                    break
            
            if found_url:
                normalized_url = found_url.rstrip('/')
                if normalized_url in seen_urls:
                    continue 
                seen_urls.add(normalized_url)
                unique_items.append(item)
            else:
                unique_items.append(item)

        if len(items) != len(unique_items):
            print(f"ğŸ” ExtractorAgent å…¨å±€å»é‡: {len(items)} -> {len(unique_items)} æ¡")
        
        return unique_items