import os
import json
import csv
import pandas as pd
import psycopg2
from psycopg2.extras import Json
from datetime import datetime
from typing import List, Dict, Any, Union

# å¼•å…¥ registry ä»¥è·å–ç¼“å­˜æ•°æ®
from agent.tools.registry import tool_registry

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
OUTPUT_DIR = "output"
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

def _get_timestamp_filename(prefix: str, extension: str) -> str:
    """ç”Ÿæˆå¸¦æœ‰æ—¶é—´æˆ³çš„æ–‡ä»¶å"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # æ¸…æ´—æ–‡ä»¶åï¼Œé˜²æ­¢éæ³•å­—ç¬¦
    clean_prefix = "".join([c if c.isalnum() else "_" for c in prefix])
    return os.path.join(OUTPUT_DIR, f"{clean_prefix}_{timestamp}.{extension}")

def _resolve_data(data: Union[Dict, List, None]) -> Union[Dict, List, None]:
    """
    ã€æ ¸å¿ƒä¿®å¤ã€‘æ•°æ®è§£æé€»è¾‘
    ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨ Registry ä¸­çš„ç¼“å­˜æ•°æ®ï¼ˆå®Œæ•´æ•°æ®æ€»çº¿ï¼‰ã€‚
    åŸå› ï¼šLLM çš„ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼Œå®ƒä¼ å…¥çš„ 'data' å‚æ•°å¾€å¾€æ˜¯è¢«æˆªæ–­çš„ï¼ˆå¦‚åªåŒ…å«å‰2æ¡ï¼‰ã€‚
    åªæœ‰å½“ç¼“å­˜ä¸ºç©ºæ—¶ï¼Œæ‰å°è¯•ä½¿ç”¨ LLM ä¼ å…¥çš„ dataã€‚
    """
    # 1. ä¼˜å…ˆæ£€æŸ¥ç¼“å­˜ (æ•°æ®æ€»çº¿)
    cached_data = tool_registry.last_execution_result
    
    if cached_data:
        print(f"ğŸ” [SaveTool] æ£€æµ‹åˆ°å†…å­˜ç¼“å­˜æ•°æ® (Type: {type(cached_data)})ã€‚")
        print("   -> ä¸ºé˜²æ­¢æ•°æ®æˆªæ–­ï¼Œå¿½ç•¥ LLM ä¼ å…¥çš„ data å‚æ•°ï¼Œå¼ºåˆ¶ä½¿ç”¨ç¼“å­˜çš„å®Œæ•´æ•°æ®ã€‚")
        return cached_data

    # 2. å¦‚æœç¼“å­˜ä¸ºç©ºï¼ˆæ¯”å¦‚ç›´æ¥è°ƒç”¨çš„ save è€Œæ²¡ç»è¿‡çˆ¬è™«ï¼‰ï¼Œæ‰ä½¿ç”¨ä¼ å…¥å‚æ•°
    if data:
        print("ğŸ” [SaveTool] ç¼“å­˜ä¸ºç©ºï¼Œä½¿ç”¨ä¼ å…¥çš„ data å‚æ•°ã€‚")
        return data
    
    return None

def save_to_json(data: Dict[str, Any] = None, filename_prefix: str = "crawl_result") -> str:
    """
    ä¿å­˜æ•°æ®ä¸º JSON æ–‡ä»¶
    å‚æ•°:
        data: (å¯é€‰) å³ä½¿ LLM ä¼ å…¥äº†æ­¤å‚æ•°ï¼Œåªè¦ä¸Šä¸€æ­¥æœ‰ç¼“å­˜ï¼Œä¹Ÿä¼šè¢«å¿½ç•¥ã€‚
        filename_prefix: æ–‡ä»¶åå‰ç¼€
    """
    # è§£ææ•°æ® (ä¼˜å…ˆå–ç¼“å­˜)
    actual_data = _resolve_data(data)
    
    if not actual_data:
        return "ä¿å­˜å¤±è´¥: æ²¡æœ‰æ•°æ®å¯ä¿å­˜ (å‚æ•°ä¸ºç©ºä¸”æ— ç¼“å­˜)"

    file_path = _get_timestamp_filename(filename_prefix, "json")
    
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(actual_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSON æ–‡ä»¶å·²ä¿å­˜: {file_path}")
        return f"æˆåŠŸä¿å­˜ JSON åˆ°: {file_path}"
    except Exception as e:
        return f"ä¿å­˜ JSON å¤±è´¥: {str(e)}"

def save_to_csv(data: Union[Dict, List] = None, filename_prefix: str = "crawl_result") -> str:
    """
    ä¿å­˜æ•°æ®ä¸º CSV æ–‡ä»¶
    """
    actual_data = _resolve_data(data)
    
    if not actual_data:
        return "ä¿å­˜å¤±è´¥: æ²¡æœ‰æ•°æ®å¯ä¿å­˜"

    file_path = _get_timestamp_filename(filename_prefix, "csv")
    
    # æå–åˆ—è¡¨æ•°æ®
    items = []
    if isinstance(actual_data, dict):
        # ä¼˜å…ˆæŸ¥æ‰¾ data æˆ– items å­—æ®µ
        items = actual_data.get("data") or actual_data.get("items") or actual_data.get("target_content") or []
        # å¦‚æœæ•°æ®æœ¬èº«å°±æ˜¯å•æ¡å­—å…¸ï¼Œä¸”ä¸åœ¨ items é‡Œ
        if not items and "url" in actual_data:
             items = [actual_data]
    elif isinstance(actual_data, list):
        items = actual_data
        
    if not items:
        return f"ä¿å­˜ CSV å¤±è´¥: æ•°æ®æ ¼å¼ä¸åŒ…å«åˆ—è¡¨é¡¹ (Keys: {list(actual_data.keys()) if isinstance(actual_data, dict) else 'List'})"

    try:
        # ä½¿ç”¨ Pandas è¿›è¡Œæ™ºèƒ½è½¬æ¢
        df = pd.DataFrame(items)
        
        # å¼ºåˆ¶å°†æ‰€æœ‰éåŸºç¡€ç±»å‹çš„åˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢ CSV å†™å…¥æŠ¥é”™
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].apply(lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, (list, dict)) else x)
        
        df.to_csv(file_path, index=False, encoding='utf-8-sig') # sig ç”¨äºè§£å†³ Excel ä¸­æ–‡ä¹±ç 
        print(f"ğŸ’¾ CSV æ–‡ä»¶å·²ä¿å­˜: {file_path} (å…± {len(df)} æ¡)")
        return f"æˆåŠŸä¿å­˜ CSV åˆ°: {file_path}"
    except Exception as e:
        return f"ä¿å­˜ CSV å¤±è´¥: {str(e)}"

def save_to_postgres(data: Union[Dict, List] = None, table_name: str = "crawled_data") -> str:
    """
    ä¿å­˜æ•°æ®åˆ° PostgreSQL æ•°æ®åº“
    """
    actual_data = _resolve_data(data)
    
    if not actual_data:
        return "ä¿å­˜å¤±è´¥: æ²¡æœ‰æ•°æ®å¯ä¿å­˜"

    # ä»ç¯å¢ƒå˜é‡è·å–æ•°æ®åº“è¿æ¥ä¸²
    dsn = os.environ.get("POSTGRES_CONNECTION_STRING")
    if not dsn:
        return "ä¿å­˜å¤±è´¥: æœªè®¾ç½® POSTGRES_CONNECTION_STRING ç¯å¢ƒå˜é‡"

    # æå–è¦ä¿å­˜çš„åˆ—è¡¨
    items = []
    source_url = ""
    
    if isinstance(actual_data, dict):
        items = actual_data.get("data") or actual_data.get("items") or actual_data.get("target_content") or []
        source_url = actual_data.get("root_url") or actual_data.get("url") or ""
    elif isinstance(actual_data, list):
        items = actual_data

    if not items:
        return "ä¿å­˜æ•°æ®åº“å¤±è´¥: æ•°æ®ä¸ºç©º"

    conn = None
    try:
        conn = psycopg2.connect(dsn)
        cur = conn.cursor()
        
        # 1. å»ºè¡¨ (å¦‚æœä¸å­˜åœ¨)
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            id SERIAL PRIMARY KEY,
            source_url TEXT,
            crawled_at TIMESTAMP DEFAULT NOW(),
            content JSONB
        );
        """
        cur.execute(create_table_sql)
        
        # 2. æ‰¹é‡æ’å…¥
        insert_sql = f"INSERT INTO {table_name} (source_url, content) VALUES (%s, %s)"
        
        count = 0
        for item in items:
            cur.execute(insert_sql, (source_url, Json(item)))
            count += 1
            
        conn.commit()
        print(f"ğŸ’¾ å·²å°† {count} æ¡æ•°æ®å­˜å…¥æ•°æ®åº“è¡¨: {table_name}")
        return f"æˆåŠŸå°† {count} æ¡æ•°æ®å­˜å…¥ PostgreSQL è¡¨ '{table_name}'"

    except Exception as e:
        if conn:
            conn.rollback()
        return f"æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}"
    finally:
        if conn:
            conn.close()