import os
import time
import json
import httpx
import uuid
import random
from typing import List, Dict, Any, Union
from dotenv import load_dotenv

# LangChain & Milvus
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import OllamaEmbeddings
from langchain_milvus import Milvus

# å¼•å…¥ registry ä»¥è·å–ç¼“å­˜æ•°æ®
from agent.tools.registry import tool_registry

from config import *

load_dotenv()

def get_embedding_model():
    """
    å·¥å‚å‡½æ•°ï¼šè‡ªåŠ¨é€‰æ‹© OpenAI æˆ– Ollama åµŒå…¥æ¨¡å‹
    """
    http_client = httpx.Client(trust_env=False, timeout=60.0)
    if EMBEDDING_TYPE == 'local_ollama':
        print(f"ğŸ”Œ ä½¿ç”¨ OllamaEmbeddings (Model: {OPENAI_OLLAMA_EMBEDDING_MODEL})...")
        # OllamaEmbeddings ä¸éœ€è¦ /v1 åç¼€
        base_url = OPENAI_OLLAMA_BASE_URL.replace("/api/generate", "").replace("/v1", "").rstrip("/")
        return OllamaEmbeddings(
            base_url=base_url,
            model=OPENAI_OLLAMA_EMBEDDING_MODEL
        )
    elif EMBEDDING_TYPE == 'local_vllm':
        print(f"ğŸ”Œ ä½¿ç”¨ Vllm OpenAIEmbeddings (Model: {VLLM_OPENAI_EMBEDDING_MODEL})...")
        return OpenAIEmbeddings(
            model=VLLM_OPENAI_EMBEDDING_MODEL,
            openai_api_key=VLLM_OPENAI_EMBEDDING_API_KEY,
            openai_api_base=VLLM_OPENAI_EMBEDDING_BASE_URL,
            http_client=http_client,
            # å…³é—­æœ¬åœ° Token æ£€æŸ¥ï¼Œå¼ºåˆ¶å‘é€çº¯æ–‡æœ¬
            check_embedding_ctx_length=False
        )
    else:
        return OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_key=OPENAI_API_KEY,
            openai_api_base=OPENAI_OLLAMA_BASE_URL
        )

def _resolve_data(data: Union[Dict, List, None]) -> Union[Dict, List, None]:
    cached_data = tool_registry.last_execution_result
    if cached_data:
        if isinstance(cached_data, str):
            print(f"âš ï¸ [SaveToKB] ç¼“å­˜æ•°æ®æ˜¯å­—ç¬¦ä¸²ï¼Œè·³è¿‡ã€‚")
        elif isinstance(cached_data, list) and len(cached_data) == 0:
            print("âš ï¸ [SaveToKB] ç¼“å­˜æ•°æ®ä¸ºç©ºã€‚")
            return cached_data
        else:
            print(f"ğŸ” [SaveToKB] ä½¿ç”¨å†…å­˜ç¼“å­˜æ•°æ®ã€‚")
            return cached_data
    return data if data else None

def _extract_items_from_structure(data: Any) -> List[Dict]:
    """
    ã€æ ¸å¿ƒé€»è¾‘ã€‘é€’å½’æŸ¥æ‰¾åµŒå¥—å­—å…¸ä¸­åŒ…å«æ•°æ®çš„åˆ—è¡¨
    è§£å†³ç±»ä¼¼ target_content -> items è¿™ç§æ·±å±‚åµŒå¥—é—®é¢˜
    """
    if isinstance(data, list):
        return data
    
    if isinstance(data, dict):
        # 1. ä¼˜å…ˆæŸ¥æ‰¾å¸¸è§çš„æ•°æ®å®¹å™¨ Key
        priority_keys = ["items", "data", "list", "target_content", "results", "products"]
        
        for key in priority_keys:
            if key in data:
                val = data[key]
                # å¦‚æœæ‰¾åˆ°åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
                if isinstance(val, list) and len(val) > 0:
                    return val
                # å¦‚æœæ˜¯å­—å…¸ï¼ˆå¦‚ target_contentï¼‰ï¼Œé€’å½’è¿›å»æ‰¾
                if isinstance(val, dict):
                    deep_items = _extract_items_from_structure(val)
                    if deep_items: 
                        return deep_items
        
        # 2. å¦‚æœå¸¸è§ Key æ²¡æ‰¾åˆ°ï¼Œå°±æŠŠ Dict æœ¬èº«å½“åšå•æ¡æ•°æ®
        # ä½†è¦æ’é™¤é‚£ç§åªåŒ…å« meta ä¿¡æ¯ï¼ˆå¦‚ code: 200ï¼‰çš„ dict
        if len(data.keys()) > 1: # è‡³å°‘æœ‰ç‚¹å†…å®¹çš„
            return [data]
            
    return []

def _generate_children_summary(children: List[Dict], max_items: int = 50) -> str:
    """
    ã€æ–°å¢ã€‘ç”Ÿæˆå­é¡¹æ•°æ®çš„æ–‡æœ¬æ‘˜è¦
    å°† children åˆ—è¡¨è½¬æ¢ä¸ºç´§å‡‘çš„æ–‡æœ¬å—ï¼Œé™„åŠ åˆ°çˆ¶æ–‡æ¡£ä¸­ã€‚
    """
    if not children:
        return ""
    
    lines = ["\nã€å…³è”çš„è¯¦ç»†å­é¡¹åˆ—è¡¨ (Children Details)ã€‘:"]
    
    # å…³é”®è¯ä¼˜åŒ–ï¼šä¼˜å…ˆå±•ç¤º é“¾æ¥ã€æ ‡é¢˜ ç­‰å¯¹ç”¨æˆ·æœ‰ç”¨çš„ä¿¡æ¯
    priority_keys = ["title", "name", "url", "link", "href", "download", "magnet", "åç§°", "é“¾æ¥", "ä¸‹è½½"]
    
    for i, child in enumerate(children[:max_items]):
        if not isinstance(child, dict): continue
        
        # æå–å…³é”®å­—æ®µ
        parts = []
        seen_values = set()  # ã€ä¼˜åŒ–ã€‘ç”¨äºå»é‡å€¼
        
        # 1. å…ˆæ‰¾ä¼˜å…ˆå­—æ®µ
        for pk in priority_keys:
            for k, v in child.items():
                if pk in k.lower() and v and isinstance(v, (str, int)):
                     val_str = str(v).strip()
                     # é˜²æ­¢åŒä¸€æ¡å­é¡¹é‡Œ "url": "http://..." å’Œ "link": "http://..." é‡å¤å‡ºç°
                     if val_str in seen_values: continue
                     if val_str and len(val_str) < 300: # é˜²æ­¢urlè¿‡é•¿
                        parts.append(f"{k}: {val_str}")
                        seen_values.add(val_str)
        
        # 2. å¦‚æœä¼˜å…ˆå­—æ®µæ²¡æ‰¾åˆ°ï¼Œç¨å¾®è¡¥å……å…¶ä»–å­—æ®µï¼ˆæ’é™¤ childrenï¼‰
        if not parts:
            for k, v in child.items():
                if k in ["children", "items", "target_content"]: continue
                val_str = str(v).strip()
                if val_str in seen_values: continue
                
                if val_str and len(val_str) < 100:
                    parts.append(f"{k}: {val_str}")
                    seen_values.add(val_str)
                    if len(parts) >= 2: break # é™åˆ¶é•¿åº¦

        # æ ¼å¼åŒ–å•è¡Œ
        if parts:
            # å»é‡ parts (è™½ç„¶ä¸Šé¢å·²æœ‰ seen_valuesï¼Œä½†ä¸ºäº†ä¿é™©)
            unique_parts = list(set(parts))
            lines.append(f"  {i+1}. " + " | ".join(unique_parts))
    
    if len(children) > max_items:
        lines.append(f"  ... (è¿˜æœ‰ {len(children) - max_items} æ¡å­é¡¹æ•°æ®æœªå±•ç¤º)")
        
    return "\n".join(lines)

def _flatten_data_to_documents(data: Union[List, Dict], category: str = "general") -> List[Document]:
    """
    é€šç”¨ç‰ˆæ•°æ®æ‰å¹³åŒ–ï¼šè‡ªé€‚åº”å„ç§å­—æ®µå
    """
    # 1. æ™ºèƒ½æå–åˆ—è¡¨æ•°æ®
    items = _extract_items_from_structure(data)
        
    if not items:
        print("âš ï¸ [Flatten] æœªæ‰¾åˆ°æœ‰æ•ˆåˆ—è¡¨æ•°æ®ã€‚")
        return []

    documents = []
    
    for item in items:
        if not isinstance(item, dict): continue
        
        # --- A. åŠ¨æ€è¯†åˆ« Title å’Œ URL ---
        title = "æœªå‘½åæ¡ç›®"
        url = ""
        
        # å¯å‘å¼å…³é”®è¯
        title_keywords = ["title", "name", "åç§°", "å", "æ ‡é¢˜", "product", "movie"]
        url_keywords = ["url", "link", "href", "é“¾æ¥", "è·³è½¬"]

        # éå†æ‰€æœ‰å­—æ®µæ¥çŒœæµ‹ Title å’Œ URL
        for k, v in item.items():
            if not isinstance(v, str): continue
            k_lower = k.lower()
            
            # çŒœ URL
            if not url and any(kw in k_lower for kw in url_keywords) and (v.startswith("http") or v.startswith("/")):
                url = v
            
            # çŒœ Title (ä¼˜å…ˆçº§ï¼šå¦‚æœ key åŒ…å« keywordï¼Œä¸” value ä¸åƒ url)
            if title == "æœªå‘½åæ¡ç›®" and any(kw in k_lower for kw in title_keywords) and len(v) < 100:
                title = v

        # å¦‚æœæ²¡çŒœåˆ° Titleï¼Œå°è¯•ç”¨ç¬¬ä¸€ä¸ªé URL çš„çŸ­å­—ç¬¦ä¸²ä½œä¸º Title
        if title == "æœªå‘½åæ¡ç›®":
            for k, v in item.items():
                if isinstance(v, str) and len(v) > 2 and len(v) < 50 and not v.startswith("http"):
                    title = v
                    break
        
        # ç”Ÿæˆä¸€ä¸ªç»„ IDï¼Œç”¨äºå…³è”çˆ¶å­æ–‡æ¡£ï¼ˆå¤‡ç”¨ï¼‰
        group_id = str(uuid.uuid4())

        # ã€æ ¸å¿ƒä¿®å¤ã€‘åŸºç¡€ Metadata (åŒ…å«æ‰€æœ‰å¯èƒ½å­—æ®µçš„é»˜è®¤å€¼ï¼Œç¡®ä¿ Schema ä¸€è‡´)
        base_metadata = {
            "source": url, 
            "title": title, 
            "category": category,
            "group_id": group_id,
            "has_children": False,  # é»˜è®¤ False
            "parent_title": ""      # é»˜è®¤ ç©ºå­—ç¬¦ä¸²
        }

        # --- B. æ„å»º Parent æ–‡æœ¬ (åŒ…å« Children æ‘˜è¦) ---
        content_parts = []
        
        # ã€ä¼˜åŒ–ã€‘1. æ˜¾å¼æ·»åŠ æœ€é‡è¦çš„æ ‡å‡†åŒ–ä¿¡æ¯ (é¿å…é‡å¤)
        if title and title != "æœªå‘½åæ¡ç›®":
            content_parts.append(f"æ ‡é¢˜: {title}")
        if url:
            content_parts.append(f"é“¾æ¥: {url}")
            
        # ã€ä¼˜åŒ–ã€‘2. å®šä¹‰é»‘åå•ï¼Œè¿‡æ»¤æ‰å·²çŸ¥çš„å†—ä½™åŒä¹‰è¯å­—æ®µ
        # è¿™äº›å­—æ®µé€šå¸¸æ˜¯ url æˆ– title çš„é‡å¤
        redundant_keys = {
            "url", "link", "href", "é“¾æ¥", "è·³è½¬é“¾æ¥", "è¯¦æƒ…é¡µé“¾æ¥", "æ–‡ç« é“¾æ¥", "full_url", "source",
            "title", "name", "åç§°", "å", "æ ‡é¢˜", "product_name", "movie_name"
        }
        
        for k, v in item.items():
            if k in ["children", "target_content", "items"] or v is None: 
                continue
                
            k_lower = k.lower()
            
            # å¦‚æœ Key åœ¨å†—ä½™é»‘åå•é‡Œï¼Œç›´æ¥è·³è¿‡ (å› ä¸ºä¸Šé¢å·²ç»æ·»åŠ äº†æ ‡å‡†åŒ–çš„"æ ‡é¢˜"å’Œ"é“¾æ¥")
            if k_lower in redundant_keys:
                continue
                
            val_str = str(v).strip()
            if not val_str: continue
            
            # äºŒæ¬¡æ£€æŸ¥ï¼šé˜²æ­¢æ¼ç½‘ä¹‹é±¼ï¼ˆKey ä¸åœ¨é»‘åå•ï¼Œä½† Value ä¸ æ ‡é¢˜æˆ–é“¾æ¥ å®Œå…¨ä¸€è‡´ï¼‰
            if val_str == title or val_str == url:
                continue
                
            content_parts.append(f"{k}: {val_str}")
        
        parent_text_body = "\n".join(content_parts)
        
        # ç”Ÿæˆ Children æ‘˜è¦å¹¶é™„åŠ åˆ° Parent æ–‡æœ¬ä¸­
        children = item.get("children", [])
        children_summary_text = ""
        if isinstance(children, list) and children:
            children_summary_text = _generate_children_summary(children)
            
        # ç»„åˆæœ€ç»ˆçš„ Parent æ–‡æœ¬
        full_parent_text = parent_text_body
        if children_summary_text:
            full_parent_text += f"\n{children_summary_text}"
        
        if full_parent_text and len(full_parent_text) > 5:
            # åˆå¹¶ metadata
            meta = base_metadata.copy()
            meta["type"] = "parent_info"
            meta["has_children"] = bool(children)
            # parent_title é»˜è®¤ä¸ºç©ºï¼Œä¿æŒä¸€è‡´
            
            doc = Document(
                page_content=full_parent_text,
                metadata=meta
            )
            documents.append(doc)
            
        # --- C. é€’å½’å¤„ç† Children (ä¾æ—§ç”Ÿæˆç‹¬ç«‹çš„å­æ–‡æ¡£ï¼Œç”¨äºç²¾ç»†æ£€ç´¢) ---
        if isinstance(children, list):
            for child in children:
                if not isinstance(child, dict): continue
                
                child_parts = []
                # ã€ä¼˜åŒ–ã€‘å­é¡¹ä¹ŸåšåŒæ ·çš„å»é‡
                seen_child_values = set()
                
                for k, v in child.items():
                    k_lower = k.lower()
                    if k_lower in redundant_keys: continue
                    
                    val_str = str(v).strip()
                    if val_str and val_str not in seen_child_values:
                        child_parts.append(f"{k}: {val_str}")
                        seen_child_values.add(val_str)
                
                if child_parts:
                    # å­æ–‡æ¡£å¸¦ä¸Šçˆ¶æ ‡é¢˜ä¸Šä¸‹æ–‡
                    child_text = f"ã€Š{title}ã€‹çš„å­é¡¹è¯¦æƒ…:\n" + "\n".join(child_parts)
                    
                    # åˆå¹¶ metadata
                    child_meta = base_metadata.copy()
                    child_meta["type"] = "child_detail"
                    child_meta["parent_title"] = title
                    # has_children é»˜è®¤ä¸º Falseï¼Œä¿æŒä¸€è‡´
                    
                    child_doc = Document(
                        page_content=child_text,
                        metadata=child_meta
                    )
                    documents.append(child_doc)
                    
    return documents

def save_to_milvus(data: Union[Dict, List] = None, category: str = "general") -> str:
    """
    å°†æ•°æ®å­˜å…¥ Milvus å‘é‡çŸ¥è¯†åº“ (ç¨³å¥ç‰ˆ)
    """
    actual_data = _resolve_data(data)
    if not actual_data:
        return "ä¿å­˜å¤±è´¥: æ²¡æœ‰æœ‰æ•ˆæ•°æ®"

    # è½¬æ¢æ•°æ®
    docs = _flatten_data_to_documents(actual_data, category=category)

    # è¿‡æ»¤ç©ºæ–‡æ¡£
    valid_docs = [d for d in docs if d.page_content and d.page_content.strip()]
    
    if not valid_docs:
        return f"ä¿å­˜å¤±è´¥: æ•°æ®è½¬æ¢åä¸ºç©º (åŸå§‹æ•°æ®ç±»å‹: {type(actual_data)})"
    
    print(f"ğŸ”„ å‡†å¤‡å¤„ç† {len(valid_docs)} æ¡æ•°æ®ç‰‡æ®µ...")

    try:
        embeddings = get_embedding_model()

        # æ‰‹åŠ¨è®¡ç®—å‘é‡ (é˜²æ­¢ 429)
        text_embeddings = []
        metadatas = []
        texts = []
        
        print(f"âš¡ å¼€å§‹è®¡ç®—å‘é‡ (Manual Embedding Mode)...")
        for i, doc in enumerate(valid_docs):
            retry_count = 0
            max_retries = 3
            
            while retry_count < max_retries:
                try:
                    clean_text = doc.page_content.replace("\n", " ")
                    # ç®€å•çš„é•¿åº¦æˆªæ–­ï¼Œé˜²æ­¢è¶…å‡º embedding æ¨¡å‹é™åˆ¶ (ä¾‹å¦‚ OpenAI æ˜¯ 8191 tokens)
                    if len(clean_text) > 30000: 
                        clean_text = clean_text[:30000]
                        
                    vector = embeddings.embed_query(clean_text)
                    
                    texts.append(doc.page_content)
                    metadatas.append(doc.metadata)
                    text_embeddings.append(vector)
                    
                    if (i + 1) % 5 == 0:
                        print(f"   -> å·²å‘é‡åŒ– {i + 1}/{len(valid_docs)} æ¡")
                    
                    time.sleep(0.05)
                    break
                    
                except Exception as e:
                    error_str = str(e)
                    if "429" in error_str:
                        wait_time = 2 ** retry_count
                        print(f"   âš ï¸ é™æµç­‰å¾… {wait_time}s...")
                        time.sleep(wait_time)
                        retry_count += 1
                    else:
                        print(f"   âŒ ç¬¬ {i} æ¡åµŒå…¥å¤±è´¥ (Fatal): {e}")
                        break
            
        if not text_embeddings:
            return "ä¿å­˜å¤±è´¥: æ‰€æœ‰æ•°æ®å‘é‡åŒ–å‡å¤±è´¥ã€‚"

        index_params = {
            "metric_type": "IP",         # æ¨è: RAGç”¨ "IP" æˆ– "COSINE"
            "index_type": "HNSW",        # ç´¢å¼•ç±»å‹
            "params": {
                "M": 16,                 # èŠ‚ç‚¹æœ€å¤§è¿æ¥æ•°
                "efConstruction": 250    # ç´¢å¼•æ„å»ºæ·±åº¦
            }
        }

        print(f"âœ… å‘é‡è®¡ç®—å®Œæˆ ({len(text_embeddings)} æ¡)ï¼Œå‡†å¤‡å­˜å…¥ Milvus...")

        # æ³¨æ„ï¼šå¦‚æœä½ ä¸æƒ³æ¯æ¬¡éƒ½æ¸…ç©ºæ—§æ•°æ®ï¼Œè¯·å°† drop_old=False
        vector_store = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": MILVUS_URI},
            collection_name=COLLECTION_NAME,
            auto_id=True,
            drop_old=False, 
            index_params=index_params
        )
        
        vector_store.add_embeddings(
            texts=texts,
            embeddings=text_embeddings,
            metadatas=metadatas
        )
        
        try:
            if hasattr(vector_store, 'col') and vector_store.col:
                vector_store.col.flush()
            elif hasattr(vector_store, 'collection') and vector_store.collection:
                vector_store.collection.flush()
        except:
            pass
        
        return f"æˆåŠŸå°† {len(text_embeddings)} æ¡æ•°æ®å­˜å…¥çŸ¥è¯†åº“ (Collection: {COLLECTION_NAME})"
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"å‘é‡æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}"