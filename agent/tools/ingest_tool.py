import os
import time
import json
import random
from typing import List, Dict, Any, Union
from dotenv import load_dotenv

# LangChain & Milvus
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import OllamaEmbeddings
from langchain_milvus import Milvus

# å¼•å…¥ registry ä»¥è·å–ç¼“å­˜æ•°æ®
from agent.tools.registry import tool_registry

from config import *

load_dotenv()

# ========================== é…ç½®åŒºåŸŸ ==========================
# MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
# COLLECTION_NAME = "spider_knowledge_base"

# # Embedding é…ç½®
# EMBEDDING_MODEL = os.environ.get("MODA_EMBEDDING_MODEL", "text-embedding-3-small")
# OPENAI_API_KEY = os.environ.get("MODA_OPENAI_API_KEY")
# OPENAI_BASE_URL = os.environ.get("MODA_OPENAI_BASE_URL")
# OPENAI_OLLAMA_BASE_URL = os.environ.get("MODA_OLLAMA_BASE_URL", OPENAI_BASE_URL)

def get_embedding_model():
    """
    å·¥å‚å‡½æ•°ï¼šè‡ªåŠ¨é€‰æ‹© OpenAI æˆ– Ollama åµŒå…¥æ¨¡å‹
    """
    if OPENAI_OLLAMA_BASE_URL and "11434" in OPENAI_OLLAMA_BASE_URL:
        print(f"ğŸ”Œ [RAG] åˆ‡æ¢è‡³ Ollama Embeddings (Model: {EMBEDDING_MODEL})...")
        base_url = OPENAI_OLLAMA_BASE_URL.replace("/api/generate", "").replace("/v1", "").rstrip("/")
        return OllamaEmbeddings(base_url=base_url, model=OPENAI_OLLAMA_EMBEDDING_MODEL)
    else:
        return OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_key=OPENAI_API_KEY,
            openai_api_base=OPENAI_BASE_URL
        )

def _resolve_data(data: Union[Dict, List, None]) -> Union[Dict, List, None]:
    cached_data = tool_registry.last_execution_result
    if cached_data:
        if isinstance(cached_data, str):
            print(f"âš ï¸ [SaveToKB] ç¼“å­˜æ•°æ®æ˜¯å­—ç¬¦ä¸²ï¼Œè·³è¿‡ã€‚")
        elif isinstance(cached_data, list) and len(cached_data) == 0:
            print("âš ï¸ [SaveToKB] ç¼“å­˜æ•°æ®ä¸ºç©ºã€‚")
            return cached_data
        else:
            print(f"ğŸ” [SaveToKB] ä½¿ç”¨å†…å­˜ç¼“å­˜æ•°æ®ã€‚")
            return cached_data
    return data if data else None

def _extract_items_from_structure(data: Any) -> List[Dict]:
    """
    ã€æ ¸å¿ƒé€»è¾‘ã€‘é€’å½’æŸ¥æ‰¾åµŒå¥—å­—å…¸ä¸­åŒ…å«æ•°æ®çš„åˆ—è¡¨
    è§£å†³ç±»ä¼¼ target_content -> items è¿™ç§æ·±å±‚åµŒå¥—é—®é¢˜
    """
    if isinstance(data, list):
        return data
    
    if isinstance(data, dict):
        # 1. ä¼˜å…ˆæŸ¥æ‰¾å¸¸è§çš„æ•°æ®å®¹å™¨ Key
        priority_keys = ["items", "data", "list", "target_content", "results", "products"]
        
        for key in priority_keys:
            if key in data:
                val = data[key]
                # å¦‚æœæ‰¾åˆ°åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
                if isinstance(val, list) and len(val) > 0:
                    return val
                # å¦‚æœæ˜¯å­—å…¸ï¼ˆå¦‚ target_contentï¼‰ï¼Œé€’å½’è¿›å»æ‰¾
                if isinstance(val, dict):
                    deep_items = _extract_items_from_structure(val)
                    if deep_items: 
                        return deep_items
        
        # 2. å¦‚æœå¸¸è§ Key æ²¡æ‰¾åˆ°ï¼Œå°±æŠŠ Dict æœ¬èº«å½“åšå•æ¡æ•°æ®
        # ä½†è¦æ’é™¤é‚£ç§åªåŒ…å« meta ä¿¡æ¯ï¼ˆå¦‚ code: 200ï¼‰çš„ dict
        if len(data.keys()) > 1: # è‡³å°‘æœ‰ç‚¹å†…å®¹çš„
            return [data]
            
    return []

def _flatten_data_to_documents(data: Union[List, Dict]) -> List[Document]:
    """
    é€šç”¨ç‰ˆæ•°æ®æ‰å¹³åŒ–ï¼šè‡ªé€‚åº”å„ç§å­—æ®µå
    """
    # 1. æ™ºèƒ½æå–åˆ—è¡¨æ•°æ®
    items = _extract_items_from_structure(data)
        
    if not items:
        print("âš ï¸ [Flatten] æœªæ‰¾åˆ°æœ‰æ•ˆåˆ—è¡¨æ•°æ®ã€‚")
        return []

    documents = []
    
    for item in items:
        if not isinstance(item, dict): continue
        
        # --- A. åŠ¨æ€è¯†åˆ« Title å’Œ URL ---
        title = "æœªå‘½åæ¡ç›®"
        url = ""
        
        # å¯å‘å¼å…³é”®è¯
        title_keywords = ["title", "name", "åç§°", "å", "æ ‡é¢˜", "product", "movie"]
        url_keywords = ["url", "link", "href", "é“¾æ¥", "è·³è½¬"]

        # éå†æ‰€æœ‰å­—æ®µæ¥çŒœæµ‹ Title å’Œ URL
        for k, v in item.items():
            if not isinstance(v, str): continue
            k_lower = k.lower()
            
            # çŒœ URL
            if not url and any(kw in k_lower for kw in url_keywords) and (v.startswith("http") or v.startswith("/")):
                url = v
            
            # çŒœ Title (ä¼˜å…ˆçº§ï¼šå¦‚æœ key åŒ…å« keywordï¼Œä¸” value ä¸åƒ url)
            if title == "æœªå‘½åæ¡ç›®" and any(kw in k_lower for kw in title_keywords) and len(v) < 100:
                title = v

        # å¦‚æœæ²¡çŒœåˆ° Titleï¼Œå°è¯•ç”¨ç¬¬ä¸€ä¸ªé URL çš„çŸ­å­—ç¬¦ä¸²ä½œä¸º Title
        if title == "æœªå‘½åæ¡ç›®":
            for k, v in item.items():
                if isinstance(v, str) and len(v) > 2 and len(v) < 50 and not v.startswith("http"):
                    title = v
                    break

        # --- B. æ„å»ºå…¨å­—æ®µæ–‡æœ¬ (Flatten All Fields) ---
        content_parts = []
        
        # ç¬¬ä¸€å±‚å­—æ®µ
        for k, v in item.items():
            # è·³è¿‡ç‰¹æ®Šå­—æ®µå’Œç©ºå€¼
            if k in ["children", "target_content", "items"] or v is None: 
                continue
            
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            val_str = str(v).strip()
            if not val_str: continue
            
            # æ ¼å¼åŒ–: "å•†å“å: æ´‹å¥¢å‘çƒ­ä¿æš–..."
            content_parts.append(f"{k}: {val_str}")
        
        parent_text = "\n".join(content_parts)
        
        if parent_text and len(parent_text) > 5:
            doc = Document(
                page_content=parent_text,
                metadata={"source": url, "title": title, "type": "parent_info"}
            )
            documents.append(doc)
            
        # --- C. é€’å½’å¤„ç† Children (å¦‚æœ‰) ---
        children = item.get("children", [])
        # æœ‰äº›ç½‘ç«™å¯èƒ½æŠŠå­åˆ—è¡¨å« sub_items ç­‰ï¼Œè¿™é‡Œå¯ä»¥æ‰©å±•
        
        if isinstance(children, list):
            for child in children:
                if not isinstance(child, dict): continue
                
                child_parts = []
                for k, v in child.items():
                    val_str = str(v).strip()
                    if val_str:
                        child_parts.append(f"{k}: {val_str}")
                
                if child_parts:
                    # å°†çˆ¶çº§ title æ‹¼æ¥åˆ°å­çº§å†…å®¹ä¸­ï¼Œä¿æŒä¸Šä¸‹æ–‡
                    child_text = f"ã€Š{title}ã€‹çš„è¯¦ç»†ä¿¡æ¯:\n" + "\n".join(child_parts)
                    child_doc = Document(
                        page_content=child_text,
                        metadata={"source": url, "title": title, "type": "child_detail"}
                    )
                    documents.append(child_doc)
                    
    return documents

def save_to_milvus(data: Union[Dict, List] = None) -> str:
    """
    å°†æ•°æ®å­˜å…¥ Milvus å‘é‡çŸ¥è¯†åº“ (ç¨³å¥ç‰ˆ)
    """
    actual_data = _resolve_data(data)
    if not actual_data:
        return "ä¿å­˜å¤±è´¥: æ²¡æœ‰æœ‰æ•ˆæ•°æ®"

    # è½¬æ¢æ•°æ®
    docs = _flatten_data_to_documents(actual_data)
    
    # è¿‡æ»¤ç©ºæ–‡æ¡£
    valid_docs = [d for d in docs if d.page_content and d.page_content.strip()]
    
    if not valid_docs:
        return f"ä¿å­˜å¤±è´¥: æ•°æ®è½¬æ¢åä¸ºç©º (åŸå§‹æ•°æ®ç±»å‹: {type(actual_data)})"
    
    print(f"ğŸ”„ å‡†å¤‡å¤„ç† {len(valid_docs)} æ¡æ•°æ®ç‰‡æ®µ...")

    try:
        embeddings = get_embedding_model()

        # æ‰‹åŠ¨è®¡ç®—å‘é‡ (é˜²æ­¢ 429)
        text_embeddings = []
        metadatas = []
        texts = []
        
        print(f"âš¡ å¼€å§‹è®¡ç®—å‘é‡ (Manual Embedding Mode)...")
        for i, doc in enumerate(valid_docs):
            retry_count = 0
            max_retries = 3
            
            while retry_count < max_retries:
                try:
                    clean_text = doc.page_content.replace("\n", " ")
                    vector = embeddings.embed_query(clean_text)
                    
                    texts.append(doc.page_content)
                    metadatas.append(doc.metadata)
                    text_embeddings.append(vector)
                    
                    if (i + 1) % 5 == 0:
                        print(f"   -> å·²å‘é‡åŒ– {i + 1}/{len(valid_docs)} æ¡")
                    
                    time.sleep(0.05)
                    break
                    
                except Exception as e:
                    error_str = str(e)
                    if "429" in error_str:
                        wait_time = 2 ** retry_count
                        print(f"   âš ï¸ é™æµç­‰å¾… {wait_time}s...")
                        time.sleep(wait_time)
                        retry_count += 1
                    else:
                        print(f"   âŒ ç¬¬ {i} æ¡åµŒå…¥å¤±è´¥ (Fatal): {e}")
                        break
            
        if not text_embeddings:
            return "ä¿å­˜å¤±è´¥: æ‰€æœ‰æ•°æ®å‘é‡åŒ–å‡å¤±è´¥ã€‚"

        print(f"âœ… å‘é‡è®¡ç®—å®Œæˆ ({len(text_embeddings)} æ¡)ï¼Œå‡†å¤‡å­˜å…¥ Milvus...")

        vector_store = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": MILVUS_URI},
            collection_name=COLLECTION_NAME,
            auto_id=True,
            drop_old=True 
        )
        
        vector_store.add_embeddings(
            texts=texts,
            embeddings=text_embeddings,
            metadatas=metadatas
        )
        
        try:
            if hasattr(vector_store, 'col') and vector_store.col:
                vector_store.col.flush()
            elif hasattr(vector_store, 'collection') and vector_store.collection:
                vector_store.collection.flush()
        except:
            pass
        
        return f"æˆåŠŸå°† {len(text_embeddings)} æ¡æ•°æ®å­˜å…¥çŸ¥è¯†åº“ (Collection: {COLLECTION_NAME})"
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"å‘é‡æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}"