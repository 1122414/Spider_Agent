import os
import time
import json
import random
from typing import List, Dict, Any, Union
from dotenv import load_dotenv

# LangChain Components
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import OllamaEmbeddings # æ–°å¢ Ollama æ”¯æŒ
from langchain_milvus import Milvus

# å¼•å…¥ registry ä»¥è·å–ç¼“å­˜æ•°æ®
from agent.tools.registry import tool_registry

load_dotenv()

# ========================== é…ç½®åŒºåŸŸ ==========================
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
COLLECTION_NAME = "spider_knowledge_base"

# Embedding é…ç½®
EMBEDDING_MODEL = os.environ.get("MODA_EMBEDDING_MODEL", "text-embedding-3-small")
OPENAI_API_KEY = os.environ.get("MODA_OPENAI_API_KEY")
OPENAI_BASE_URL = os.environ.get("MODA_OPENAI_BASE_URL")

# æœ¬åœ° Ollama
OPENAI_OLLAMA_EMBEDDING_MODEL = os.environ.get("OPENAI_OLLAMA_EMBEDDING_MODEL", "text-embedding-3-small")
OPENAI_OLLAMA_BASE_URL = os.environ.get("OPENAI_OLLAMA_BASE_URL", OPENAI_BASE_URL)

def get_embedding_model():
    """
    å·¥å‚å‡½æ•°ï¼šè‡ªåŠ¨é€‰æ‹© OpenAI æˆ– Ollama åµŒå…¥æ¨¡å‹
    """
    # ç®€å•çš„è‡ªåŠ¨åˆ¤å®šé€»è¾‘ï¼šå¦‚æœ Base URL åŒ…å« 11434 (Ollama é»˜è®¤ç«¯å£)ï¼Œåˆ™ä½¿ç”¨ OllamaEmbeddings
    if OPENAI_OLLAMA_BASE_URL and "11434" in OPENAI_OLLAMA_BASE_URL:
        print(f"ğŸ”Œ æ£€æµ‹åˆ°æœ¬åœ° Ollama ç¯å¢ƒï¼Œåˆ‡æ¢è‡³ OllamaEmbeddings (Model: {EMBEDDING_MODEL})...")
        # OllamaEmbeddings ä¸éœ€è¦ /v1 åç¼€
        base_url = OPENAI_OLLAMA_BASE_URL.replace("/v1", "").strip("/")
        return OllamaEmbeddings(
            base_url=base_url,
            model=EMBEDDING_MODEL
        )
    else:
        # é»˜è®¤ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼
        return OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_key=OPENAI_API_KEY,
            openai_api_base=OPENAI_BASE_URL
        )

def _resolve_data(data: Union[Dict, List, None]) -> Union[Dict, List, None]:
    cached_data = tool_registry.last_execution_result
    if cached_data:
        if isinstance(cached_data, str):
            print(f"âš ï¸ [SaveToKB] ç¼“å­˜æ•°æ®æ˜¯å­—ç¬¦ä¸²ï¼Œè·³è¿‡ã€‚")
        elif isinstance(cached_data, list) and len(cached_data) == 0:
            print("âš ï¸ [SaveToKB] ç¼“å­˜æ•°æ®ä¸ºç©ºã€‚")
            return cached_data
        else:
            print(f"ğŸ” [SaveToKB] ä½¿ç”¨å†…å­˜ç¼“å­˜æ•°æ®ã€‚")
            return cached_data
    return data if data else None

def _flatten_data_to_documents(data: Union[List, Dict]) -> List[Document]:
    items = []
    if isinstance(data, dict):
        items = data.get("data") or data.get("items") or data.get("target_content") or []
        if not items and "url" in data: items = [data]
    elif isinstance(data, list):
        items = data
        
    if not items: return []

    documents = []
    for item in items:
        if not isinstance(item, dict): continue
        
        title = item.get("ç”µå½±åç§°") or item.get("title") or item.get("name") or "æœªçŸ¥æ ‡é¢˜"
        url = item.get("é“¾æ¥") or item.get("url") or item.get("link") or ""
        
        content_parts = []
        for k, v in item.items():
            if k not in ["children", "url", "link", "href", "è·³è½¬é“¾æ¥"] and v and isinstance(v, str) and v.strip():
                content_parts.append(f"{k}: {v.strip()}")
        
        parent_text = "\n".join(content_parts)
        
        if parent_text and len(parent_text.strip()) > 5:
            doc = Document(
                page_content=parent_text,
                metadata={"source": url, "title": title, "type": "parent_info"}
            )
            documents.append(doc)
            
        children = item.get("children", [])
        if isinstance(children, list):
            for child in children:
                if not isinstance(child, dict): continue
                child_parts = []
                for k, v in child.items():
                    if v and isinstance(v, str) and v.strip():
                        child_parts.append(f"{k}: {v.strip()}")
                
                if child_parts:
                    child_text = f"ã€Š{title}ã€‹çš„è¯¦ç»†ä¿¡æ¯:\n" + "\n".join(child_parts)
                    if child_text and len(child_text.strip()) > 5:
                        child_doc = Document(
                            page_content=child_text,
                            metadata={"source": url, "title": title, "type": "child_detail"}
                        )
                        documents.append(child_doc)
    return documents

def save_to_milvus(data: Union[Dict, List] = None) -> str:
    """
    å°†æ•°æ®å­˜å…¥ Milvus å‘é‡çŸ¥è¯†åº“ (æ”¯æŒ OpenAI/Ollama)
    """
    actual_data = _resolve_data(data)
    if not actual_data:
        return "ä¿å­˜å¤±è´¥: æ²¡æœ‰æœ‰æ•ˆæ•°æ®"

    docs = _flatten_data_to_documents(actual_data)
    valid_docs = [d for d in docs if d.page_content and d.page_content.strip()]
    
    if not valid_docs:
        return "ä¿å­˜å¤±è´¥: æ•°æ®è½¬æ¢åä¸ºç©º"
    
    print(f"ğŸ”„ å‡†å¤‡å¤„ç† {len(valid_docs)} æ¡æ•°æ®ç‰‡æ®µ...")

    try:
        # 1. åˆå§‹åŒ– Embedding æ¨¡å‹ (è‡ªåŠ¨åˆ¤æ–­ç±»å‹)
        embeddings = get_embedding_model()

        # 2. æ‰‹åŠ¨è®¡ç®—å‘é‡ (å¸¦é‡è¯•æœºåˆ¶)
        text_embeddings = []
        metadatas = []
        texts = []
        
        print(f"âš¡ å¼€å§‹è®¡ç®—å‘é‡ (Manual Embedding Mode)...")
        for i, doc in enumerate(valid_docs):
            retry_count = 0
            max_retries = 3
            
            while retry_count < max_retries:
                try:
                    clean_text = doc.page_content.replace("\n", " ")
                    
                    # embed_query æ˜¯æ‰€æœ‰ LangChain Embedding ç±»éƒ½æ”¯æŒçš„æ ‡å‡†æ¥å£
                    vector = embeddings.embed_query(clean_text)
                    
                    texts.append(doc.page_content)
                    metadatas.append(doc.metadata)
                    text_embeddings.append(vector)
                    
                    if (i + 1) % 5 == 0:
                        print(f"   -> å·²å‘é‡åŒ– {i + 1}/{len(valid_docs)} æ¡")
                    
                    # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦ sleepï¼Œä½†ä¸ºäº†ä¿é™©èµ·è§ä¿ç•™å¾®å°å»¶è¿Ÿ
                    time.sleep(1)
                    break
                    
                except Exception as e:
                    error_str = str(e)
                    # åªæœ‰ API è°ƒç”¨æ‰ä¼šæœ‰ 429ï¼Œæœ¬åœ°æ¨¡å‹é€šå¸¸æ˜¯å…¶ä»–é”™è¯¯
                    if "429" in error_str:
                        wait_time = 2 ** retry_count
                        print(f"   âš ï¸ é™æµç­‰å¾… {wait_time}s...")
                        time.sleep(wait_time)
                        retry_count += 1
                    else:
                        print(f"   âŒ ç¬¬ {i} æ¡åµŒå…¥å¤±è´¥ (Fatal): {e}")
                        break
            
        if not text_embeddings:
            return "ä¿å­˜å¤±è´¥: æ‰€æœ‰æ•°æ®å‘é‡åŒ–å‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹é…ç½®ã€‚"

        print(f"âœ… å‘é‡è®¡ç®—å®Œæˆ ({len(text_embeddings)} æ¡)ï¼Œå‡†å¤‡å­˜å…¥ Milvus...")

        # 3. å­˜å…¥ Milvus
        vector_store = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": MILVUS_URI},
            collection_name=COLLECTION_NAME,
            auto_id=True,
            drop_old=True  # å¼ºåˆ¶é‡å»ºè¡¨ä»¥é€‚åº”æ–°æ¨¡å‹çš„ç»´åº¦
        )
        
        # å­˜å…¥æ•°æ®
        vector_store.add_embeddings(
            texts=texts,
            embeddings=text_embeddings,
            metadatas=metadatas
        )
        
        # å¼ºåˆ¶åˆ·æ–°
        try:
            if hasattr(vector_store, 'col') and vector_store.col:
                vector_store.col.flush()
            elif hasattr(vector_store, 'collection') and vector_store.collection:
                vector_store.collection.flush()
        except:
            pass
        
        return f"æˆåŠŸå°† {len(text_embeddings)} æ¡æ•°æ®å­˜å…¥çŸ¥è¯†åº“ (Collection: {COLLECTION_NAME})"
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"å‘é‡æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}"