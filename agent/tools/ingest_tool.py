import os
import json
from typing import List, Dict, Any, Union
from dotenv import load_dotenv

# LangChain & Milvus
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_milvus import Milvus

# å¼•å…¥ registry ä»¥è·å–ç¼“å­˜æ•°æ®
from agent.tools.registry import tool_registry

load_dotenv()

# é…ç½®
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
COLLECTION_NAME = "spider_knowledge_base"
EMBEDDING_MODEL = os.environ.get("MODA_EMBEDDING_MODEL", "text-embedding-3-small")
OPENAI_API_KEY = os.environ.get("MODA_OPENAI_API_KEY")
OPENAI_BASE_URL = os.environ.get("MODA_OPENAI_BASE_URL")

def _resolve_data(data: Union[Dict, List, None]) -> Union[Dict, List, None]:
    """è§£ææ•°æ®ï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
    cached_data = tool_registry.last_execution_result
    if cached_data:
        if isinstance(cached_data, str):
            print(f"âš ï¸ [SaveToKB] ç¼“å­˜æ•°æ®æ˜¯å­—ç¬¦ä¸²ï¼Œè·³è¿‡ã€‚")
        elif isinstance(cached_data, list) and len(cached_data) == 0:
            print("âš ï¸ [SaveToKB] ç¼“å­˜æ•°æ®ä¸ºç©ºã€‚")
            return cached_data
        else:
            print(f"ğŸ” [SaveToKB] ä½¿ç”¨å†…å­˜ç¼“å­˜æ•°æ®ã€‚")
            return cached_data
    return data if data else None

def _flatten_data_to_documents(data: Union[List, Dict]) -> List[Document]:
    """å°†æ ‘çŠ¶ç»“æ„çš„çˆ¬è™«æ•°æ®æ‰å¹³åŒ–ä¸º Document å¯¹è±¡åˆ—è¡¨"""
    items = []
    if isinstance(data, dict):
        items = data.get("data") or data.get("items") or data.get("target_content") or []
        if not items and "url" in data: items = [data]
    elif isinstance(data, list):
        items = data
        
    if not items: return []

    documents = []
    for item in items:
        if not isinstance(item, dict): continue
        
        # 1. æå–å…ƒæ•°æ®
        title = item.get("ç”µå½±åç§°") or item.get("title") or item.get("name") or "æœªçŸ¥æ ‡é¢˜"
        url = item.get("é“¾æ¥") or item.get("url") or item.get("link") or ""
        
        # 2. æ„å»ºçˆ¶çº§æ–‡æœ¬
        content_parts = []
        for k, v in item.items():
            # è¿‡æ»¤æ‰ç©ºå€¼å’Œéæ–‡æœ¬å€¼
            if k not in ["children", "url", "link", "href", "è·³è½¬é“¾æ¥"] and v and isinstance(v, str) and v.strip():
                content_parts.append(f"{k}: {v.strip()}")
        
        parent_text = "\n".join(content_parts)
        
        # ä¸¥æ ¼è¿‡æ»¤ç©ºæ–‡æœ¬
        if parent_text and len(parent_text.strip()) > 5:
            doc = Document(
                page_content=parent_text,
                metadata={"source": url, "title": title, "type": "parent_info"}
            )
            documents.append(doc)
            
        # 3. å¤„ç† Children
        children = item.get("children", [])
        if isinstance(children, list):
            for child in children:
                if not isinstance(child, dict): continue
                child_parts = []
                for k, v in child.items():
                    if v and isinstance(v, str) and v.strip():
                        child_parts.append(f"{k}: {v.strip()}")
                
                if child_parts:
                    child_text = f"ã€Š{title}ã€‹çš„è¯¦ç»†ä¿¡æ¯:\n" + "\n".join(child_parts)
                    if child_text and len(child_text.strip()) > 5:
                        child_doc = Document(
                            page_content=child_text,
                            metadata={"source": url, "title": title, "type": "child_detail"}
                        )
                        documents.append(child_doc)
    return documents

def save_to_milvus(data: Union[Dict, List] = None) -> str:
    """
    å°†æ•°æ®å­˜å…¥ Milvus å‘é‡çŸ¥è¯†åº“ (Dockerç‰ˆ)
    """
    actual_data = _resolve_data(data)
    if not actual_data:
        return "ä¿å­˜å¤±è´¥: æ²¡æœ‰æœ‰æ•ˆæ•°æ®"

    docs = _flatten_data_to_documents(actual_data)
    
    # æœ€åä¸€é“é˜²çº¿ï¼šç¡®ä¿æ²¡æœ‰ç©º Document
    valid_docs = [d for d in docs if d.page_content and d.page_content.strip()]
    
    if not valid_docs:
        return "ä¿å­˜å¤±è´¥: æ•°æ®è½¬æ¢åä¸ºç©ºï¼Œæ— æ³•å…¥åº“"
    
    print(f"ğŸ”„ å‡†å¤‡å°† {len(valid_docs)} æ¡æ•°æ®ç‰‡æ®µå­˜å…¥ Milvus ({MILVUS_URI})...")

    try:
        # åˆå§‹åŒ– Embedding
        # ç§»é™¤ chunk_size å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®ï¼Œé¿å…è§¦å‘ IndexError
        embeddings = OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_key=OPENAI_API_KEY,
            openai_api_base=OPENAI_BASE_URL
        )

        # è¿æ¥ Milvus
        vector_store = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": MILVUS_URI},
            collection_name=COLLECTION_NAME,
            auto_id=True,
            drop_old=False
        )
        
        # ã€æ ¸å¿ƒä¿®å¤ã€‘æ‰‹åŠ¨åˆ†æ‰¹å†™å…¥
        # è§„é¿ langchain-openai åœ¨å¤„ç†å¤§åˆ—è¡¨æ—¶çš„ IndexError bug
        # æ¯æ¬¡åªå†™ 50 æ¡ï¼Œç¨³å¥æ€§æé«˜
        BATCH_SIZE = 50
        total_batches = (len(valid_docs) + BATCH_SIZE - 1) // BATCH_SIZE
        
        print(f"ğŸ“¦ å¼€å§‹åˆ†æ‰¹å†™å…¥ (Batch Size: {BATCH_SIZE}, Total Batches: {total_batches})...")
        
        for i in range(0, len(valid_docs), BATCH_SIZE):
            batch = valid_docs[i : i + BATCH_SIZE]
            vector_store.add_documents(batch)
            print(f"   -> Batch {i // BATCH_SIZE + 1}/{total_batches} å®Œæˆ ({len(batch)} docs)")
        
        print(f"ğŸ’¾ å…¨éƒ¨å®Œæˆï¼æˆåŠŸå°† {len(valid_docs)} ä¸ªçŸ¥è¯†ç‰‡æ®µå­˜å…¥ Milvusã€‚")
        return f"æˆåŠŸå°† {len(valid_docs)} æ¡æ•°æ®å­˜å…¥çŸ¥è¯†åº“ (Milvus Collection: {COLLECTION_NAME})"
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"å‘é‡æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}"