import os
import re
import html
import json
from typing import List, Dict, Any, Set, Union
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_community.document_transformers import Html2TextTransformer

from agent.tools.dom_helper import dom_analyzer
from agent.prompt_template import XPATH_ANALYSIS_PROMPT
from agent.prompt_template import SCRAWL_DATA_SYSTEM_PROMPT

from config import *

load_dotenv()

class ExtractorAgent:
    def __init__(self):
        self.llm = ChatOpenAI(
            model=MODEL_NAME, 
            temperature=0.1, # é™ä½æ¸©åº¦ä»¥æé«˜æ ¼å¼ç¨³å®šæ€§
            openai_api_key=OPENAI_API_KEY, 
            openai_api_base=OPENAI_BASE_URL
        )

    def get_content(self, fetched_html: str, target: List[str], source: str) -> Dict[str, Any]:
        """
        æ ¹æ® HTML å’Œç›®æ ‡å­—æ®µï¼Œä½¿ç”¨ LLM æå–ç»“æ„åŒ–æ•°æ®
        Return: {"items": List[Dict], "next_page_url": str | None}
        """
        # 1. é¢„æ£€æŸ¥
        if not fetched_html or len(fetched_html.strip()) < 10:
            print("âš ï¸ è­¦å‘Š: è¾“å…¥çš„ HTML å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œè·³è¿‡æå–ã€‚")
            return {"items": [], "next_page_url": None}

        # ============================================================
        # åˆ†å—ç­–ç•¥ (Map-Reduce)
        # ============================================================
        CHUNK_SIZE = 20000  # 20k å­—ç¬¦å®‰å…¨é˜ˆå€¼
        
        # Fast Path: ä¸åˆ†å—
        if len(fetched_html) <= CHUNK_SIZE:
            return self._process_single_chunk(fetched_html, target, source)

        # Slow Path: åˆ†å—å¤„ç†
        print(f"ğŸ“¦ å†…å®¹è¿‡é•¿ ({len(fetched_html)} chars)ï¼Œå¯åŠ¨åˆ†å—æå– (Chunk Size: {CHUNK_SIZE})...")
        chunks = self._split_text_by_lines(fetched_html, CHUNK_SIZE)
        print(f"   -> åˆ‡åˆ†ä¸º {len(chunks)} å—ï¼Œå¼€å§‹é€å—æå–...")

        all_items = []
        detected_next_page = None
        
        for i, chunk in enumerate(chunks):
            # æå–å½“å‰å—
            chunk_result = self._process_single_chunk(chunk, target, source)
            
            # 1. æ”¶é›† items
            items = chunk_result.get("items", [])
            if items:
                all_items.extend(items)
                print(f"âœ… ç¬¬ {i+1} å—æå–åˆ° {len(items)} æ¡æ•°æ®")
            
            # 2. æ”¶é›† next_page_url 
            # ç¿»é¡µé“¾æ¥é€šå¸¸åœ¨é¡µé¢çš„åº•éƒ¨ï¼ˆå³æœ€åå‡ ä¸ªå—ä¸­ï¼‰ï¼Œä½†ä¹Ÿå¯èƒ½åœ¨ä¸­é—´ï¼ˆå¦‚â€œæ›´å¤šâ€æŒ‰é’®ï¼‰
            # ç­–ç•¥ï¼šåªè¦å‘ç°æœ‰æ•ˆç¿»é¡µé“¾æ¥ï¼Œå°±è®°å½•ä¸‹æ¥ã€‚åç»­å—å¦‚æœå‘ç°æ–°çš„ï¼Œå¯ä»¥è¦†ç›–ï¼ˆå‡è®¾åº•éƒ¨çš„æ˜¯çœŸæ­£çš„ä¸‹ä¸€é¡µï¼‰
            # æˆ–è€…ï¼šä¼˜å…ˆä¿ç•™åŒ…å« "page" æˆ–æ•°å­—çš„é“¾æ¥
            if chunk_result.get("next_page_url"):
                new_next = chunk_result["next_page_url"]
                # ç®€å•çš„å»é‡/ä¼˜å…ˆçº§é€»è¾‘ï¼šå¦‚æœä¹‹å‰æ²¡æ‰¾åˆ°ï¼Œæˆ–è€…æ–°æ‰¾åˆ°çš„çœ‹èµ·æ¥æ›´åƒåˆ†é¡µ
                if not detected_next_page:
                    detected_next_page = new_next
                    print(f"      ğŸ” ç¬¬ {i+1} å—å‘ç°äº†ç¿»é¡µé“¾æ¥: {detected_next_page}")
                elif new_next != detected_next_page:
                    # å¦‚æœè¿™å—ä¹Ÿæ‰¾åˆ°äº†ä¸ä¸€æ ·çš„é“¾æ¥ï¼Œå¯èƒ½æ˜¯åº•éƒ¨çš„"ä¸‹ä¸€é¡µ"è¦†ç›–äº†ä¸­é—´çš„"æ›´å¤š"
                    # é€šå¸¸åº•éƒ¨çš„ä¼˜å…ˆçº§æ›´é«˜
                    detected_next_page = new_next
                    print(f"      ğŸ”„ ç¬¬ {i+1} å—æ›´æ–°äº†ç¿»é¡µé“¾æ¥: {detected_next_page}")

        print(f"ğŸ“¦ åˆ†å—æå–å®Œæˆï¼ŒåŸå§‹æ€»æ¡æ•°: {len(all_items)}")

        # å…¨å±€å»é‡
        final_items = self._deduplicate_items(all_items)
        
        return {
            "items": final_items,
            "next_page_url": detected_next_page
        }
    
    def _try_extract_next_page_by_regex(self, text: str) -> Union[str, None]:
        """
        ã€æ–°å¢ã€‘æ­£åˆ™å…œåº•æå–ï¼šå½“ LLM å¿½ç•¥æ—¶ï¼Œæš´åŠ›ä» Markdown ä¸­æŸ¥æ‰¾å¯¼èˆªé“¾æ¥
        é’ˆå¯¹: [æ›´å¤š __](https://...) æˆ– [ä¸‹ä¸€é¡µ](...)
        """
        # å…³é”®è¯ï¼šæ›´å¤š, Next, ä¸‹ä¸€é¡µ, ä¸‹é¡µ, More, >>, Â»
        keywords = r"(æ›´å¤š|Next|ä¸‹ä¸€é¡µ|ä¸‹é¡µ|More|>>|Â»)"
        
        # Regex è§£é‡Š:
        # \[\s* åŒ¹é… [ å’Œç©ºç™½
        # ([^\]]*?keywords[^\]]*?) åŒ¹é…åŒ…å«å…³é”®è¯çš„æ–‡æœ¬ (Group 1: Link Text)
        # \s*\]           åŒ¹é… ] å’Œç©ºç™½
        # \((https?://[^)]+)\)     åŒ¹é… (URL) (Group 2: URL)
        
        pattern = re.compile(r'\[\s*([^\]]*?' + keywords + r'[^\]]*?)\s*\]\((https?://[^)]+)\)', re.IGNORECASE)
        
        matches = pattern.findall(text)
        if matches:
            # å¯èƒ½ä¼šåŒ¹é…åˆ°å¤šä¸ªï¼Œæ¯”å¦‚ [æ›´å¤šç”µå½±] [æ›´å¤šæ–°é—»]
            # ç­–ç•¥ï¼šä¼˜å…ˆè¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…åˆ°çš„æœ‰æ•ˆ HTTP é“¾æ¥
            for link_text, kw, url in matches:
                # æ’é™¤æ˜æ˜¾æ— å…³çš„é“¾æ¥
                if "APP" in link_text or "ä¸‹è½½" in link_text:
                    continue
                return url
        return None

    def _process_single_chunk(self, chunk_text: str, target: List[str], source: str) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªå—ï¼Œè¿”å› {"items": [], "next_page_url": ...}
        """
        prompt = PromptTemplate.from_template(SCRAWL_DATA_SYSTEM_PROMPT)
        
        try:
            # user_query è½¬å­—ç¬¦ä¸²ï¼Œé¿å…ç”±åˆ—è¡¨å¼•å‘æ ¼å¼é—®é¢˜
            resp = self.llm.invoke(prompt.format(user_query=str(target), summary=chunk_text, source=source))
            content = resp.content.strip()
        except Exception as e:
            print(f"âŒ LLM Chunk Error: {e}")
            return {"items": [], "next_page_url": None}

        # è§£æ JSON
        raw_result = self._parse_json_safely(content)
        
        # æ ¼å¼æ ‡å‡†åŒ–ï¼šç¡®ä¿è¿”å›ç»“æ„æ˜¯ {"items": [], "next_page_url": None}
        final_structure = {"items": [], "next_page_url": None}

        if isinstance(raw_result, dict):
            # æƒ…å†µ A: æ ‡å‡†è¿”å›
            if "items" in raw_result:
                final_structure["items"] = raw_result["items"] if isinstance(raw_result["items"], list) else []
                final_structure["next_page_url"] = raw_result.get("next_page_url")
            # æƒ…å†µ B: æ—§æ ¼å¼å•ä¸ªå¯¹è±¡
            elif "items" not in raw_result and "error" not in raw_result: 
                 final_structure["items"] = [raw_result]

        elif isinstance(raw_result, list):
            # æƒ…å†µ C: çº¯åˆ—è¡¨
            final_structure["items"] = raw_result
        
        # ============================================================
        # ã€å…³é”®ä¿®å¤ã€‘æ­£åˆ™å…œåº•æ£€æµ‹ç¿»é¡µé“¾æ¥
        # ============================================================
        if not final_structure.get("next_page_url"):
            fallback_url = self._try_extract_next_page_by_regex(chunk_text)
            if fallback_url:
                print(f"ğŸ” [Regex Fallback] LLMæœªè¯†åˆ«ï¼Œä½†æ­£åˆ™æå–åˆ°ç¿»é¡µé“¾æ¥: {fallback_url}")
                final_structure["next_page_url"] = fallback_url

        return final_structure

    def _split_text_by_lines(self, text: str, max_length: int) -> List[str]:
        """æŒ‰è¡Œåˆ‡åˆ†æ–‡æœ¬ï¼Œå¹¶å®‰å…¨å¤„ç†è¶…é•¿è¡Œ"""
        lines = text.split('\n')
        chunks = []
        current_chunk = []
        current_length = 0
        
        for line in lines:
            line_len = len(line) + 1 
            
            if line_len > max_length:
                if current_chunk:
                    chunks.append("\n".join(current_chunk))
                    current_chunk = []
                    current_length = 0
                
                while len(line) > max_length:
                    chunks.append(line[:max_length])
                    line = line[max_length:]
                
                current_chunk = [line]
                current_length = len(line) + 1
                continue 

            if current_length + line_len > max_length:
                chunks.append("\n".join(current_chunk))
                current_chunk = [line]
                current_length = line_len
            else:
                current_chunk.append(line)
                current_length += line_len
        
        if current_chunk:
            chunks.append("\n".join(current_chunk))
            
        return chunks

    def _parse_json_safely(self, text: str) -> Union[List, Dict]:
        """å®‰å…¨è§£æ JSON"""
        try:
            return json.loads(text)
        except:
            pass

        cleaned = text.replace("```json", "").replace("```", "").strip()
        try:
            return json.loads(cleaned)
        except:
            pass

        try:
            match = re.search(r'\{[\s\S]*\}', text) 
            if match:
                return json.loads(match.group(0))
        except:
            pass

        try:
            match = re.search(r'\[[\s\S]*\]', text)
            if match:
                return json.loads(match.group(0))
        except:
            pass

        return {"items": [], "next_page_url": None, "error": "Parse Failed"}

    def _deduplicate_items(self, items: List[Dict]) -> List[Dict]:
        """ç»“æœå»é‡"""
        if not items: return []
        unique_items = []
        seen_urls = set()
        target_keys = ["url", "link", "href", "é“¾æ¥", "è¯¦æƒ…é¡µé“¾æ¥", "æ–‡ç« é“¾æ¥", "full_url"]

        for item in items:
            if not isinstance(item, dict):
                unique_items.append(item)
                continue
            
            found_url = None
            for k, v in item.items():
                if k.lower() in target_keys and v and isinstance(v, str):
                    found_url = v.strip()
                    break
            
            if found_url:
                normalized = found_url.rstrip('/')
                if normalized in seen_urls: continue
                seen_urls.add(normalized)
                unique_items.append(item)
            else:
                unique_items.append(item)
        
        if len(items) != len(unique_items):
            print(f"ğŸ” ExtractorAgent å…¨å±€å»é‡: {len(items)} -> {len(unique_items)} æ¡")
        return unique_items