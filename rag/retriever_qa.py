import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_milvus import Milvus
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# é…ç½®
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
COLLECTION_NAME = "spider_knowledge_base"
EMBEDDING_MODEL = os.environ.get("MODA_EMBEDDING_MODEL", "text-embedding-3-small")
MODEL_NAME = os.environ.get("MODA_MODEL_NAME", "gpt-4o-mini")
OPENAI_API_KEY = os.environ.get("MODA_OPENAI_API_KEY")
OPENAI_BASE_URL = os.environ.get("MODA_OPENAI_BASE_URL")

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def qa_interaction(question: str) -> str:
    print(f"ğŸ¤” RAG Searching for: {question}")
    
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL,
        openai_api_key=OPENAI_API_KEY,
        openai_api_base=OPENAI_BASE_URL
    )
    
    llm = ChatOpenAI(
        model=MODEL_NAME,
        temperature=0,
        openai_api_key=OPENAI_API_KEY,
        openai_api_base=OPENAI_BASE_URL
    )

    try:
        # è¿æ¥ Milvus (Docker)
        vector_store = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": MILVUS_URI},
            collection_name=COLLECTION_NAME,
        )
        
        # æ£€ç´¢ Top 3
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})

        template = """ä½ æ˜¯ä¸€ä¸ªåŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„ã€ä¸Šä¸‹æ–‡ã€‘å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´â€œçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ã€‚

        ã€ä¸Šä¸‹æ–‡ã€‘:
        {context}

        ã€é—®é¢˜ã€‘:
        {question}

        ã€å›ç­”ã€‘:"""
        
        custom_rag_prompt = PromptTemplate.from_template(template)

        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | custom_rag_prompt
            | llm
            | StrOutputParser()
        )

        result = rag_chain.invoke(question)
        return result

    except Exception as e:
        return f"RAG ç³»ç»Ÿå‡ºé”™: {str(e)}"