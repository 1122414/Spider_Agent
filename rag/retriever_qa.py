import os
import sys
import torch
import httpx
import traceback
from typing import List, Tuple
from dotenv import load_dotenv

# LangChain ç›¸å…³
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.embeddings import OllamaEmbeddings
from langchain_milvus import Milvus
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document

# Transformers ç›¸å…³ (Qwen Reranker)
from transformers import AutoTokenizer, AutoModelForCausalLM

# è‡ªå®šä¹‰æ¨¡å— (å‡è®¾è¿™äº›åœ¨ä½ çš„é¡¹ç›®ä¸­å­˜åœ¨)
from config import * 
from rag.query_analyzer import query_analyzer
from agent.prompt_template import RAG_PROMPT

load_dotenv()

# ==============================================================================
# 1. é…ç½®åŒºåŸŸ
# ==============================================================================
# æ˜¾å­˜ä¼˜åŒ–é…ç½®
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
RERANK_MODEL_PATH = r"G:\models\Qwen\Qwen3-Reranker-4B"
RERANK_MAX_LENGTH = 8192

# ==============================================================================
# 2. æ ¸å¿ƒç±»ï¼šQwenReranker (å°è£…å®˜æ–¹é€»è¾‘)
# ==============================================================================

class QwenReranker:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            print(f"ğŸš€ [System] Loading Qwen3-Reranker-4B on {DEVICE}...")
            cls._instance = super(QwenReranker, cls).__new__(cls)
            cls._instance._init_model()
        return cls._instance

    def _init_model(self):
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                RERANK_MODEL_PATH, 
                padding_side='left', 
                trust_remote_code=True
            )
            
            # è‡ªåŠ¨é€‰æ‹©ç²¾åº¦ï¼Œæ˜¾å­˜å……è¶³å»ºè®® float16ï¼Œä¸”ä½¿ç”¨ flash_attention_2
            model_kwargs = {
                "device_map": DEVICE,
                "trust_remote_code": True
            }
            if DEVICE == "cuda":
                model_kwargs["torch_dtype"] = torch.float16
                # å¦‚æœå®‰è£…äº† flash-attn åº“ï¼Œå–æ¶ˆä¸‹é¢æ³¨é‡Šä»¥è·å¾—åŠ é€Ÿ
                # model_kwargs["attn_implementation"] = "flash_attention_2"
            
            self.model = AutoModelForCausalLM.from_pretrained(
                RERANK_MODEL_PATH, 
                **model_kwargs
            ).eval()

            self.token_false_id = self.tokenizer.convert_tokens_to_ids("no")
            self.token_true_id = self.tokenizer.convert_tokens_to_ids("yes")
            
            # Prompt æ¨¡æ¿æ„å»ºå—
            self.prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
            self.suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
            self.prefix_tokens = self.tokenizer.encode(self.prefix, add_special_tokens=False)
            self.suffix_tokens = self.tokenizer.encode(self.suffix, add_special_tokens=False)

        except Exception as e:
            print(f"âŒ [Error] Failed to load Qwen Reranker: {e}")
            raise e

    def _format_instruction(self, query: str, doc_content: str, instruction: str = None) -> str:
        if instruction is None:
            instruction = 'Given a web search query, retrieve relevant passages that answer the query'
        return "<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}".format(
            instruction=instruction, query=query, doc=doc_content
        )

    def _process_inputs(self, pairs: List[str]):
        # å®˜æ–¹é€»è¾‘ï¼šæ‰‹åŠ¨æ‹¼æ¥ Token å¹¶å¤„ç† Padding
        inputs = self.tokenizer(
            pairs, 
            padding=False, 
            truncation='longest_first',
            return_attention_mask=False, 
            max_length=RERANK_MAX_LENGTH - len(self.prefix_tokens) - len(self.suffix_tokens)
        )
        
        # æ‹¼æ¥ prefix å’Œ suffix
        for i, ele in enumerate(inputs['input_ids']):
            inputs['input_ids'][i] = self.prefix_tokens + ele + self.suffix_tokens
            
        # Pad åˆ°åŒä¸€é•¿åº¦
        inputs = self.tokenizer.pad(inputs, padding=True, return_tensors="pt", max_length=RERANK_MAX_LENGTH)
        
        # ç§»åŠ¨åˆ° GPU
        for key in inputs:
            inputs[key] = inputs[key].to(self.model.device)
            
        return inputs

    @torch.no_grad()
    def rerank(self, query: str, docs: List[Document], top_k: int = 5) -> List[Document]:
        """
        æ ¸å¿ƒé‡æ’åºæ–¹æ³•
        """
        if not docs:
            return []

        # 1. å‡†å¤‡è¾“å…¥å¯¹
        doc_contents = [doc.page_content for doc in docs]
        pairs = [self._format_instruction(query, content) for content in doc_contents]

        # 2. Tokenize & Process
        inputs = self._process_inputs(pairs)

        # 3. æ¨ç† (Compute Logits)
        batch_scores = self.model(**inputs).logits[:, -1, :]
        true_vector = batch_scores[:, self.token_true_id]
        false_vector = batch_scores[:, self.token_false_id]
        
        # Stack & Log Softmax
        batch_scores = torch.stack([false_vector, true_vector], dim=1)
        batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)
        
        # è·å– "yes" çš„æ¦‚ç‡ä½œä¸ºåˆ†æ•°
        scores = batch_scores[:, 1].exp().tolist()

        # 4. æ’åºå¹¶ç»„åˆç»“æœ
        doc_score_pairs = list(zip(docs, scores))
        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

        print(f"ğŸ“Š [Rerank] Top score: {doc_score_pairs[0][1]:.4f} | Low score: {doc_score_pairs[-1][1]:.4f}")
        
        # è¿”å› Top K æ–‡æ¡£
        return [doc for doc, _ in doc_score_pairs[:top_k]]

# ==============================================================================
# 3. è¾…åŠ©å‡½æ•°
# ==============================================================================

def get_embedding_model():
    """è‡ªåŠ¨é€‰æ‹© OpenAI æˆ– Ollama Embeddings"""
    http_client = httpx.Client(trust_env=False, timeout=60.0)
    if EMBEDDING_TYPE == 'local_ollama':
        print(f"ğŸ”Œ ä½¿ç”¨ OllamaEmbeddings (Model: {OPENAI_OLLAMA_EMBEDDING_MODEL})...")
        base_url = OPENAI_OLLAMA_BASE_URL.replace("/api/generate", "").replace("/v1", "").rstrip("/")
        return OllamaEmbeddings(base_url=base_url, model=OPENAI_OLLAMA_EMBEDDING_MODEL)
    elif EMBEDDING_TYPE == 'local_vllm':
        return OpenAIEmbeddings(
            model=VLLM_OPENAI_EMBEDDING_MODEL,
            openai_api_key=VLLM_OPENAI_EMBEDDING_API_KEY,
            openai_api_base=VLLM_OPENAI_EMBEDDING_BASE_URL,
            http_client=http_client,
            check_embedding_ctx_length=False
        )
    else:
        return OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_key=OPENAI_API_KEY,
            openai_api_base=OPENAI_OLLAMA_BASE_URL
        )

def format_docs(docs):
    return "\n\n".join(f"[ç‰‡æ®µ {i+1}] {doc.page_content}" for i, doc in enumerate(docs))

def get_retrieval_k(question: str) -> int:
    global_keywords = ["å…¨éƒ¨", "æ‰€æœ‰", "åˆ—è¡¨", "æ¸…å•", "æ€»ç»“", "åˆ†æ", "all", "summary"]
    if any(kw in question.lower() for kw in global_keywords):
        return 20 # æ€»ç»“ç±»é—®é¢˜éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡
    return 10     # äº‹å®ç±»é—®é¢˜

# ==============================================================================
# 4. ä¸»ä¸šåŠ¡é€»è¾‘ (Hybrid: Vector Search + Qwen Rerank)
# ==============================================================================

def qa_interaction(question: str) -> str:
    print(f"ğŸ¤” RAG Searching for: {question}")
    
    embeddings = get_embedding_model()
    generated_expr = query_analyzer.generate_expr(question)
    
    llm = ChatOpenAI(
        model=MODEL_NAME,
        temperature=0.1,
        openai_api_key=OPENAI_API_KEY,
        openai_api_base=OPENAI_BASE_URL
    )

    try:
        os.environ.pop("http_proxy", None)
        os.environ.pop("https_proxy", None)
        # --- Step 1: è¿æ¥ Milvus ---
        vector_store = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": MILVUS_URI},
            collection_name=COLLECTION_NAME,
            consistency_level="Bounded",
            auto_id=True,
        )
        
        # --- Step 2: å‘é‡ç²—æ’ (Recall) ---
        target_k = get_retrieval_k(question)
        # æ‰©å¤§å¬å›æ± ï¼Œç»™ Reranker è¶³å¤Ÿçš„é€‰æ‹©ç©ºé—´ (å»ºè®® 5-10 å€ target_k)
        recall_k = target_k * 5 
        
        print(f"ğŸ” [Retrieve] Fetching Top-{recall_k} candidates from Milvus...")
        
        # ä½¿ç”¨ MMR å¢åŠ å¤šæ ·æ€§ï¼Œé˜²æ­¢å¬å›è¿‡äºç›¸ä¼¼çš„å†…å®¹
        retriever = vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": recall_k,
                "expr": generated_expr,
                "fetch_k": recall_k * 2,
                "lambda_mult": 0.6
            }
        )
        
        # æ˜¾å¼æ‰§è¡Œæ£€ç´¢ (LCEL Chain å¾ˆéš¾æ’å…¥é‡æ’åºï¼Œæ‰€ä»¥è¿™é‡Œæ–­å¼€ Chain æ‰‹åŠ¨æ‰§è¡Œ)
        initial_docs = retriever.invoke(question)
        if not initial_docs:
            return "æ²¡æœ‰åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # --- Step 3: ç²¾æ’ (Rerank with Qwen) ---
        print(f"âš–ï¸ [Rerank] Re-ranking {len(initial_docs)} docs using Qwen3-Reranker-4B...")
        reranker = QwenReranker() # è·å–å•ä¾‹
        final_docs = reranker.rerank(question, initial_docs, top_k=target_k)

        # --- Step 4: ç”Ÿæˆ (Generate) ---
        if RAG_PROMPT:
            if isinstance(RAG_PROMPT, str):
                custom_rag_prompt = PromptTemplate.from_template(RAG_PROMPT)
            else:
                custom_rag_prompt = RAG_PROMPT
        else:
            template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚\n\nä¸Šä¸‹æ–‡ï¼š\n{context}\n\né—®é¢˜ï¼š{question}"""
            custom_rag_prompt = PromptTemplate.from_template(template)

        # æ„å»ºæœ€ç»ˆä¸Šä¸‹æ–‡
        formatted_context = format_docs(final_docs)
        
        # æ‰‹åŠ¨æ‰§è¡Œ Chain çš„æœ€åä¸€æ­¥
        chain = (
            custom_rag_prompt
            | llm
            | StrOutputParser()
        )
        
        response = chain.invoke({"context": formatted_context, "question": question})
        return response

    except Exception as e:
        traceback.print_exc()
        return f"RAG ç³»ç»Ÿå‡ºé”™: {str(e)}"

if __name__ == "__main__":
    # ç®€å•çš„å‘½ä»¤è¡Œæµ‹è¯•
    q = sys.argv[1] if len(sys.argv) > 1 else "æµ‹è¯•ï¼šä»‹ç»ä¸€ä¸‹ Qwen Reranker çš„ä¼˜åŠ¿ï¼Ÿ"
    print(qa_interaction(q))