import os
import sys
import torch
import httpx
import traceback
from typing import List, Tuple, Dict, Any, Optional
from dotenv import load_dotenv

# LangChain ç›¸å…³
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.embeddings import OllamaEmbeddings
from langchain_milvus import Milvus
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun

# --- æ··åˆæ£€ç´¢ç›¸å…³ ---
from langchain_community.retrievers import BM25Retriever

# Transformers ç›¸å…³ (ç”¨äº Rerank)
from transformers import AutoTokenizer, AutoModelForCausalLM

# é¡¹ç›®å†…éƒ¨æ¨¡å—
from config import *
from rag.query_analyzer import query_analyzer
from agent.prompt_template import RAG_PROMPT

# ==============================================================================
# 0. å…¨å±€é…ç½®ä¸è®¾å¤‡æ£€æµ‹
# ==============================================================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
RERANK_MAX_LENGTH = 2048

# ==============================================================================
# 1. QwenReranker (é‡æ’åºæ¨¡å‹å°è£…)
# ==============================================================================
class QwenReranker:
    """
    ä½¿ç”¨ Qwen (æˆ–å…¼å®¹æ¶æ„) æ¨¡å‹è¿›è¡Œæ–‡æ¡£é‡æ’åº (Reranking)ã€‚
    å•ä¾‹æ¨¡å¼ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹ã€‚
    """
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            print(f"ğŸš€ [System] Initializing QwenReranker on {DEVICE}...")
            cls._instance = super(QwenReranker, cls).__new__(cls)
            cls._instance.model = None
            cls._instance.tokenizer = None
            try:
                cls._instance._init_model()
            except Exception as e:
                print(f"âŒ [Reranker] Model load failed: {e}")
                print("   -> Tip: Ensure 'transformers' and 'torch' are installed and RERANK_MODEL_PATH is correct.")
        return cls._instance
    
    def _init_model(self):
        # å»¶è¿ŸåŠ è½½ï¼ŒèŠ‚çœèµ„æº
        self.tokenizer = AutoTokenizer.from_pretrained(
            RERANK_MODEL_PATH, 
            padding_side='left', 
            trust_remote_code=True
        )
        
        model_kwargs = {"device_map": DEVICE, "trust_remote_code": True}
        if DEVICE == "cuda":
            # æ˜¾å­˜ä¼˜åŒ–
            model_kwargs["torch_dtype"] = torch.float16
        
        self.model = AutoModelForCausalLM.from_pretrained(
            RERANK_MODEL_PATH, 
            **model_kwargs
        ).eval()
        
        # é’ˆå¯¹ Qwen Instruct æ¨¡å‹çš„æ‰“åˆ† Token ID (Yes/No)
        # æ³¨æ„ï¼šä¸åŒæ¨¡å‹çš„ token id ä¸åŒï¼Œè¿™é‡Œé€‚é… Qwen/Qwen2
        self.token_false_id = self.tokenizer.convert_tokens_to_ids("no")
        self.token_true_id = self.tokenizer.convert_tokens_to_ids("yes")
        
        # æ„é€  Instruct Prompt (å‚è€ƒ BGE-Reranker-V2-Gemma æˆ–ç±»ä¼¼ Instruct Reranker)
        self.prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query. Answer 'yes' or 'no'.<|im_end|>\n<|im_start|>user\n"
        self.suffix = "<|im_end|>\n<|im_start|>assistant\n"
        
        self.prefix_tokens = self.tokenizer.encode(self.prefix, add_special_tokens=False)
        self.suffix_tokens = self.tokenizer.encode(self.suffix, add_special_tokens=False)

    def _format_input(self, query: str, doc_content: str) -> str:
        return f"Query: {query}\nDocument: {doc_content[:1000]}" # æˆªæ–­é˜²æ­¢OOM

    @torch.no_grad()
    def rerank(self, query: str, docs: List[Document], top_k: int = 5) -> List[Document]:
        if not docs or not self.model: 
            return docs[:top_k]
            
        # æ„é€  Batch Input
        pairs = []
        for doc in docs:
            text = self._format_input(query, doc.page_content)
            pairs.append(text)

        # Tokenize
        inputs = self.tokenizer(
            pairs, 
            padding=True, 
            truncation=True,
            return_tensors="pt", 
            max_length=RERANK_MAX_LENGTH
        ).to(self.model.device)

        # Forward pass (åªè®¡ç®— logitsï¼Œä¸ç”Ÿæˆ)
        outputs = self.model(**inputs)
        logits = outputs.logits[:, -1, :] # å–æœ€åä¸€ä¸ª token çš„ logits
        
        # è®¡ç®— Yes çš„æ¦‚ç‡
        # è¿™é‡Œæ¼”ç¤ºå– yes token çš„ log_softmax
        scores = logits[:, self.token_true_id].float().cpu().numpy()
        
        # æ’åº
        doc_score_pairs = list(zip(docs, scores))
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
        
        print(f"ğŸ“Š [Rerank] Top score: {doc_score_pairs[0][1]:.4f} | Low score: {doc_score_pairs[-1][1]:.4f}")
        
        return [doc for doc, _ in doc_score_pairs[:top_k]]

# ==============================================================================
# 2. æ ¸å¿ƒè¾…åŠ©å‡½æ•°
# ==============================================================================

def get_embedding_model():
    """è‡ªåŠ¨é€‰æ‹© OpenAI æˆ– Ollama Embeddings"""
    http_client = httpx.Client(trust_env=False, timeout=60.0)
    
    if EMBEDDING_TYPE == 'local_ollama':
        # æ¸…æ´— base_url
        base_url = OPENAI_OLLAMA_BASE_URL.replace("/api/generate", "").replace("/v1", "").rstrip("/")
        return OllamaEmbeddings(base_url=base_url, model=OPENAI_OLLAMA_EMBEDDING_MODEL)
        
    elif EMBEDDING_TYPE == 'local_vllm':
        return OpenAIEmbeddings(
            model=VLLM_OPENAI_EMBEDDING_MODEL,
            openai_api_key=VLLM_OPENAI_EMBEDDING_API_KEY,
            openai_api_base=VLLM_OPENAI_EMBEDDING_BASE_URL,
            http_client=http_client,
            check_embedding_ctx_length=False
        )
    else:
        return OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_key=OPENAI_API_KEY,
            openai_api_base=OPENAI_OLLAMA_BASE_URL
        )

def format_docs(docs):
    """æ ¼å¼åŒ–æ–‡æ¡£åˆ—è¡¨ä¸ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
    return "\n\n".join(f"[ç‰‡æ®µ {i+1}] {doc.page_content}" for i, doc in enumerate(docs))

def get_retrieval_k(question: str) -> int:
    """æ ¹æ®é—®é¢˜ç±»å‹åŠ¨æ€è°ƒæ•´ Top-K"""
    global_keywords = ["å…¨éƒ¨", "æ‰€æœ‰", "åˆ—è¡¨", "æ¸…å•", "æ€»ç»“", "åˆ†æ", "all", "summary", "list"]
    if any(kw in question.lower() for kw in global_keywords):
        return 50
    return 20    

# ==============================================================================
# 3. æ··åˆæ£€ç´¢æ„å»ºå™¨
# ==============================================================================

class SimpleEnsembleRetriever(BaseRetriever):
    """
    æ‰‹åŠ¨å®ç°çš„æ··åˆæ£€ç´¢å™¨ï¼Œç”¨äºæ›¿ä»£ langchain.retrievers.EnsembleRetriever
    ä½¿ç”¨åŠ æƒå€’æ•°æ’å (RRF) ç®—æ³•åˆå¹¶ç»“æœã€‚
    """
    retrievers: List[BaseRetriever]
    weights: List[float]

    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun = None
    ) -> List[Document]:
        
        # 1. å¹¶è¡Œæˆ–ä¸²è¡Œæ‰§è¡Œæ‰€æœ‰æ£€ç´¢å™¨
        # (ç®€å•å®ç°ä¸ºä¸²è¡Œï¼Œç”Ÿäº§ç¯å¢ƒå¯ç”¨ asyncio.gather)
        doc_lists = []
        for i, retriever in enumerate(self.retrievers):
            try:
                docs = retriever.invoke(
                    query, 
                    config={"callbacks": run_manager.get_child() if run_manager else None}
                )
                doc_lists.append(docs)
            except Exception as e:
                print(f"âš ï¸ [SimpleEnsemble] Retriever {i} failed: {e}")
                doc_lists.append([])

        # 2. RRF (Reciprocal Rank Fusion) èåˆç®—æ³•
        # æ ¸å¿ƒæ€æƒ³ï¼šæ’åè¶Šé å‰ï¼Œåˆ†æ•°è¶Šé«˜ (1 / (rank + c))
        c = 60 # RRF å¸¸æ•°ï¼Œé€šå¸¸è®¾ä¸º 60
        scores = {}
        
        for docs, weight in zip(doc_lists, self.weights):
            for rank, doc in enumerate(docs):
                # ä½¿ç”¨å†…å®¹ä½œä¸º Key è¿›è¡Œå»é‡ (Milvusè¿”å›çš„IDå¯èƒ½ä¸ä¸€è‡´)
                # æ›´å¥½çš„åšæ³•æ˜¯ç”¨ hash(doc.page_content)ï¼Œè¿™é‡Œç›´æ¥ç”¨å†…å®¹å­—ç¬¦ä¸²
                key = doc.page_content
                
                if key not in scores:
                    scores[key] = {"doc": doc, "score": 0.0}
                
                # åŠ æƒåˆ†æ•°ç´¯åŠ 
                scores[key]["score"] += weight * (1 / (rank + c))
        
        # 3. æ ¹æ®æœ€ç»ˆ RRF åˆ†æ•°æ’åº
        sorted_results = sorted(scores.values(), key=lambda x: x["score"], reverse=True)
        
        # 4. è¿”å› Document å¯¹è±¡åˆ—è¡¨
        return [item["doc"] for item in sorted_results]

def build_hybrid_retriever(milvus_store: Milvus, expr: str, k: int):
    """
    æ„å»ºæ··åˆæ£€ç´¢å™¨ï¼šMilvus (Dense) + BM25 (Sparse)
    """
    # 1. å‡†å¤‡ Milvus æ£€ç´¢å™¨ (Dense - è¯­ä¹‰æ£€ç´¢)
    milvus_retriever = milvus_store.as_retriever(
        search_type="mmr", # ä½¿ç”¨ MMR å¢åŠ å¤šæ ·æ€§
        search_kwargs={
            "k": k,
            "expr": expr, # æ³¨å…¥ query_analyzer ç”Ÿæˆçš„ expr
            "fetch_k": k * 2,
            "lambda_mult": 0.6
        }
    )

    # 2. å‡†å¤‡ BM25 æ£€ç´¢å™¨ (Sparse - å…³é”®è¯æ£€ç´¢)
    print("â³ [Hybrid] æ„å»ºä¸´æ—¶ BM25 ç´¢å¼• (In-Memory)...")
    bm25_retriever = None
    
    try:
        # å°è¯•ä» Milvus æ‹‰å–æ•°æ®æ„å»º BM25
        # âš ï¸ æ³¨æ„ï¼šä»…é€‚ç”¨äºæ•°æ®é‡ < 50k çš„åœºæ™¯ã€‚æµ·é‡æ•°æ®è¯·ä½¿ç”¨ Milvus 2.4+ çš„ Sparse Vector æˆ– ElasticSearch
        if milvus_store.col:
            # æ‹‰å–é™åˆ¶ï¼šé˜²æ­¢å†…å­˜æº¢å‡ºï¼Œæ‹‰å–æœ€æ–°çš„ 2000 æ¡æ„å»ºå…³é”®è¯ç´¢å¼•
            res = milvus_store.col.query(
                expr="pk >= 0", 
                output_fields=["text", "source", "title", "category", "type"], 
                limit=2000,
                offset=0
            )
            
            if res:
                bm25_docs = []
                for r in res:
                    # é‡å»º Document å¯¹è±¡
                    meta = {
                        "source": r.get("source"),
                        "title": r.get("title"),
                        "category": r.get("category"),
                        "type": r.get("type")
                    }
                    # Milvus LangChain é»˜è®¤æŠŠ content å­˜åœ¨ 'text' å­—æ®µ
                    text_content = r.get("text") or r.get("page_content") or ""
                    if text_content:
                        bm25_docs.append(Document(page_content=text_content, metadata=meta))
                
                if bm25_docs:
                    bm25_retriever = BM25Retriever.from_documents(bm25_docs)
                    bm25_retriever.k = k # è®¾ç½® BM25 çš„å¬å›æ•°é‡
                    print(f"   -> BM25 ç´¢å¼•æ„å»ºå®Œæˆ (Docs: {len(bm25_docs)})")
                else:
                    print("   -> Milvus è¿”å›æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ BM25")
            else:
                print("   -> æ— æ³•ä» Milvus æ‹‰å–æ•°æ®ï¼Œè·³è¿‡ BM25")
                
    except Exception as e:
        print(f"âš ï¸ [Hybrid] BM25 æ„å»ºå¤±è´¥ (é™çº§ä¸ºçº¯å‘é‡æ£€ç´¢): {e}")

    # 3. ç»„åˆ (Custom Ensemble)
    if bm25_retriever:
        print("ğŸ”— [Hybrid] å¯ç”¨æ··åˆæ£€ç´¢: Milvus(0.5) + BM25(0.5)")
        # ä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„ SimpleEnsembleRetriever
        return SimpleEnsembleRetriever(
            retrievers=[milvus_retriever, bm25_retriever],
            weights=[0.5, 0.5] # æƒé‡å¯è°ƒï¼Œ0.5/0.5 æ˜¯æ¯”è¾ƒå‡è¡¡çš„èµ·ç‚¹
        )
    else:
        print("âš ï¸ [Hybrid] ä»…ä½¿ç”¨ Milvus å‘é‡æ£€ç´¢")
        return milvus_retriever

# ==============================================================================
# 4. RAG ä¸»æµç¨‹
# ==============================================================================

def qa_interaction(question: str) -> str:
    print(f"\nğŸ¤” [RAG] Searching for: {question}")
    
    # A. æ„å›¾åˆ†æ (ç”Ÿæˆ SQL/Expr)
    expr = ""
    if query_analyzer:
        expr = query_analyzer.generate_expr(question)
    
    embeddings = get_embedding_model()
    
    llm = ChatOpenAI(
        model=MODEL_NAME,
        temperature=0.1,
        openai_api_key=OPENAI_API_KEY,
        openai_api_base=OPENAI_BASE_URL
    )

    try:
        # B. è¿æ¥ Milvus
        vector_store = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": MILVUS_URI},
            collection_name=COLLECTION_NAME,
            consistency_level="Bounded",
            auto_id=True,
        )
        
        # C. æ··åˆæ£€ç´¢ (Recall)
        target_k = get_retrieval_k(question)
        recall_k = target_k * 3  # å¬å› 3 å€æ•°é‡ç»™ Reranker ç­›é€‰
        
        hybrid_retriever = build_hybrid_retriever(vector_store, expr, recall_k)
        
        print(f"ğŸ” [Retrieve] Fetching candidates...")
        initial_docs = hybrid_retriever.invoke(question)
        
        if not initial_docs:
            return "âŒ æ²¡æœ‰åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        # D. å»é‡ (Deduplicate)
        # EnsembleRetriever å¯èƒ½ä¼šè¿”å›é‡å¤æ–‡æ¡£ (å¦‚æœ BM25 å’Œ Vector éƒ½å‘½ä¸­äº†åŒä¸€ä¸ª)
        unique_docs = []
        seen_content = set()
        for doc in initial_docs:
            # ä½¿ç”¨å†…å®¹æŒ‡çº¹å»é‡
            fingerprint = doc.page_content[:100]
            if fingerprint not in seen_content:
                unique_docs.append(doc)
                seen_content.add(fingerprint)
        
        print(f"   -> Retrieved {len(unique_docs)} unique docs (from {len(initial_docs)} raw).")

        # E. ç²¾æ’ (Rerank)
        print(f"âš–ï¸ [Rerank] ä½¿ç”¨ QwenReranker è¿›è¡Œç²¾æ’...")
        try:
            reranker = QwenReranker()
            final_docs = reranker.rerank(question, unique_docs, top_k=target_k)
        except Exception as e:
            print(f"âš ï¸ Rerank failed: {e}, using raw retrieval results.")
            final_docs = unique_docs[:target_k]

        # F. ç”Ÿæˆ (Generate)
        # å‡†å¤‡ Prompt
        if RAG_PROMPT:
            if isinstance(RAG_PROMPT, str):
                custom_rag_prompt = PromptTemplate.from_template(RAG_PROMPT)
            else:
                custom_rag_prompt = RAG_PROMPT
        else:
            # é»˜è®¤ Prompt
            template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚\n\nä¸Šä¸‹æ–‡ï¼š\n{context}\n\né—®é¢˜ï¼š{question}"""
            custom_rag_prompt = PromptTemplate.from_template(template)

        formatted_context = format_docs(final_docs)
        
        print("ğŸ“ [Generate] Generating answer...")
        chain = (
            custom_rag_prompt
            | llm
            | StrOutputParser()
        )
        
        response = chain.invoke({"context": formatted_context, "question": question})
        return response

    except Exception as e:
        traceback.print_exc()
        return f"RAG ç³»ç»Ÿè‡´å‘½é”™è¯¯: {str(e)}"

if __name__ == "__main__":
    # æµ‹è¯•å…¥å£
    q = sys.argv[1] if len(sys.argv) > 1 else "æµ‹è¯•ï¼šä»‹ç»ä¸€ä¸‹ç³»ç»Ÿé‡Œçš„ç”µå½±"
    print(qa_interaction(q))