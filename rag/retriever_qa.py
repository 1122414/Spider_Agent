import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_milvus import Milvus
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from agent.prompt_template import RAG_PROMPT

load_dotenv()

# é…ç½®
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
COLLECTION_NAME = "spider_knowledge_base"
EMBEDDING_MODEL = os.environ.get("MODA_EMBEDDING_MODEL", "text-embedding-3-small")
MODEL_NAME = os.environ.get("MODA_MODEL_NAME", "gpt-4o-mini")
OPENAI_API_KEY = os.environ.get("MODA_OPENAI_API_KEY")
OPENAI_BASE_URL = os.environ.get("MODA_OPENAI_BASE_URL")

def format_docs(docs):
    # åœ¨åˆå¹¶æ–‡æ¡£æ—¶ï¼Œç»™æ¯ä¸ªç‰‡æ®µåŠ ä¸ªåºå·ï¼Œæ–¹ä¾¿æ¨¡å‹å¼•ç”¨
    return "\n\n".join(f"[ç‰‡æ®µ {i+1}] {doc.page_content}" for i, doc in enumerate(docs))

def determine_search_kwargs(question: str) -> dict:
    """
    ã€æ ¸å¿ƒé€»è¾‘ã€‘æ ¹æ®é—®é¢˜ç±»å‹ï¼ŒåŠ¨æ€è°ƒæ•´æ£€ç´¢ç­–ç•¥ (Dynamic K)
    """
    # å®šä¹‰è§¦å‘â€œå…¨å±€æ£€ç´¢â€çš„å…³é”®è¯
    global_keywords = ["å…¨éƒ¨", "æ‰€æœ‰", "åˆ—è¡¨", "æ¸…å•", "æ€»ç»“", "åˆ†æ", "æ¦‚æ‹¬", "all", "summary", "list", "æœ‰å“ªäº›", "ç»Ÿè®¡"]
    
    # æ£€æŸ¥é—®é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯
    is_global_query = any(kw in question.lower() for kw in global_keywords)
    
    if is_global_query:
        print("ğŸš€ æ£€æµ‹åˆ°å…¨å±€/æ€»ç»“æ€§æé—®ï¼Œå¯åŠ¨ã€å¼ºåŠ›æ£€ç´¢æ¨¡å¼ã€‘(k=100)...")
        # GPT-4o-mini ä¸Šä¸‹æ–‡å¾ˆå¤§(128k)ï¼Œå¯ä»¥è½»æ¾å¤„ç† 100 ä¸ªç‰‡æ®µ (çº¦ 2w token)
        # è¿™æ ·å°±èƒ½ä¸€æ¬¡æ€§æŠŠå‡ åéƒ¨ç”µå½±çš„ä¿¡æ¯éƒ½å–‚ç»™æ¨¡å‹ï¼Œè®©å®ƒåšæ€»ç»“
        return {"k": 100} 
    else:
        print("ğŸ” æ£€æµ‹åˆ°å…·ä½“äº‹å®æé—®ï¼Œä½¿ç”¨ã€ç²¾å‡†æ£€ç´¢æ¨¡å¼ã€‘(k=10)...")
        return {"k": 10}

def qa_interaction(question: str) -> str:
    print(f"ğŸ¤” RAG Searching for: {question}")
    
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL,
        openai_api_key=OPENAI_API_KEY,
        openai_api_base=OPENAI_BASE_URL
    )
    
    llm = ChatOpenAI(
        model=MODEL_NAME,
        temperature=0.1, # æ€»ç»“ç±»ä»»åŠ¡é™ä½æ¸©åº¦ï¼Œå‡å°‘ç¼–é€ 
        openai_api_key=OPENAI_API_KEY,
        openai_api_base=OPENAI_BASE_URL
    )

    try:
        vector_store = Milvus(
            embedding_function=embeddings,
            connection_args={"uri": MILVUS_URI},
            collection_name=COLLECTION_NAME,
        )
        
        # 1. åŠ¨æ€å†³å®š k å€¼
        # å¦‚æœä½ é—®â€œæœ‰å“ªäº›ç”µå½±â€ï¼Œè¿™é‡Œå°±ä¼šè®© Milvus è¿”å›æœ€ç›¸å…³çš„ 100 æ¡æ•°æ®
        # åŸºæœ¬ä¸Šå°±èƒ½è¦†ç›–ä½ çˆ¬å–çš„æ‰€æœ‰ç”µå½±äº†
        search_kwargs = determine_search_kwargs(question)
        retriever = vector_store.as_retriever(search_kwargs=search_kwargs)

        # 2. å¢å¼ºç‰ˆ Prompt
        # æ˜ç¡®å‘Šè¯‰æ¨¡å‹å®ƒå°†æ”¶åˆ°å¤§é‡æ•°æ®ï¼Œéœ€è¦è¿›è¡Œç»¼åˆå¤„ç†
        template = RAG_PROMPT
        
        custom_rag_prompt = PromptTemplate.from_template(template)

        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | custom_rag_prompt
            | llm
            | StrOutputParser()
        )

        result = rag_chain.invoke(question)
        return result

    except Exception as e:
        return f"RAG ç³»ç»Ÿå‡ºé”™: {str(e)}"